<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="è®¡é‡ç»æµå­¦ç¬”è®°, Frank&#39;s Blog, LDVYYC, äºé›¨ç›, blog">
    <meta name="baidu-site-verification" content="fmlEuI34ir" />
    <meta name="google-site-verification" content="yCy2azpds5XSuGZvis6OuA-XIGF5GuGpYRAaGfD6o48" />
    <meta name="360-site-verification" content="b7c11a830ef90fd1464ad6206bb7b6e7" />
    <meta name="description" content="Ch1 Introductorywhat is econometrics
Combine statistical techniques with economic theory. 
Estimating economic relations">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>è®¡é‡ç»æµå­¦ç¬”è®° | Frank&#39;s Blog</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">
    <style type="text/css">
        
    </style>

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
    <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?ce84511d3df71640a9378a69f6293044";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

    
        <script>
            (function(){
                var bp = document.createElement('script');
                var curProtocol = window.location.protocol.split(':')[0];
                if (curProtocol === 'https') {
                    bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
                }
                else {
                    bp.src = 'http://push.zhanzhang.baidu.com/push.js';
                }
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(bp, s);
            })();
        </script>
    

    <script>
        (function(){
        var src = "https://jspassport.ssl.qhimg.com/11.0.1.js?d182b3f28525f2db83acfaaf6e696dba";
        document.write('<script src="' + src + '" id="sozz"><\/script>');
        })();
    </script>

<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Frank's Blog" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

<body>

    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/favicon.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Frank's Blog</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>é¦–é¡µ</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>æ ‡ç­¾</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>åˆ†ç±»</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>å½’æ¡£</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>å…³äº</span>
        </a>
    </li>
    
    <li>
        <a href="#searchModal" class="modal-trigger waves-effect waves-light">
            <i id="searchIcon" class="fa fa-search" title="æœç´¢"></i>
        </a>
    </li>
</ul>

<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/favicon.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Frank's Blog</div>
        <div class="logo-desc">
            
            Still I Rise
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                é¦–é¡µ
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                æ ‡ç­¾
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                åˆ†ç±»
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                å½’æ¡£
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                å…³äº
            </a>
        </li>
        
        
    </ul>
</div>

        </div>

        
    </nav>

</header>

    
<script src="/libs/cryptojs/crypto-js.min.js"></script>
<script>
    (function() {
        let pwd = '';
        if (pwd && pwd.length > 0) {
            if (pwd !== CryptoJS.SHA256(prompt('è¯·è¾“å…¥è®¿é—®æœ¬æ–‡ç« çš„å¯†ç ')).toString(CryptoJS.enc.Hex)) {
                alert('å¯†ç é”™è¯¯ï¼Œå°†è¿”å›ä¸»é¡µï¼');
                location.href = '/';
            }
        }
    })();
</script>




<div class="bg-cover pd-header post-cover" style="background-image: url('https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230410114258.png')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        è®¡é‡ç»æµå­¦ç¬”è®°
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 20px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                        <a href="/tags/Finance/" target="_blank">
                            <span class="chip bg-color">Finance</span>
                        </a>
                        
                        <a href="/tags/Notes/" target="_blank">
                            <span class="chip bg-color">Notes</span>
                        </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                        <a href="/categories/Learning/" class="post-category" target="_blank">
                            Learning
                        </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2023-04-10
                </div>

                <div class="post-author info-break-policy">
                    <i class="fa fa-user-o fa-fw"></i>ä½œè€…:&nbsp;&nbsp;
                    
                    Frank Yu
                    
                </div>

                
                
                <div class="info-break-policy">
                    <i class="fa fa-file-word-o fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    13.2k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="fa fa-clock-o fa-fw"></i>é˜…è¯»æ—¶é•¿:&nbsp;&nbsp;
                    75 åˆ†
                </div>
                
                

                
                <div id="busuanzi_container_page_pv" class="info-break-policy">
                    <i class="fa fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                    <span id="busuanzi_value_page_pv"></span>
                </div>
                
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="Ch1-Introductory"><a href="#Ch1-Introductory" class="headerlink" title="Ch1 Introductory"></a>Ch1 Introductory</h2><h3 id="what-is-econometrics"><a href="#what-is-econometrics" class="headerlink" title="what is econometrics"></a>what is econometrics</h3><ul>
<li>Combine statistical techniques with economic theory. </li>
<li>Estimating economic relationships.</li>
<li>testing economic theories.</li>
<li>evaluating and implementing government and business policy.</li>
</ul>
<h3 id="basic-types"><a href="#basic-types" class="headerlink" title="basic types"></a>basic types</h3><h4 id="Descriptive"><a href="#Descriptive" class="headerlink" title="Descriptive"></a>Descriptive</h4><ul>
<li>challenges<ul>
<li>Sampling<ul>
<li>draw conclosion about the population based on sample.</li>
</ul>
</li>
<li>Summary statistics<ul>
<li>nice way to summarize complicated data.</li>
</ul>
</li>
</ul>
</li>
<li>If we have data we would know the answer.</li>
<li>Conditional Expectations: if I condition X to be some value, what is the expect value of Y. Often a variable that can take on a very large number of values is treated as continuous for convenient.</li>
<li>Example<ul>
<li><em>Mothers smoke one more cigarette during pregnancy are expected to give birth to child with 15g lower birth weight.</em></li>
</ul>
</li>
</ul>
<h4 id="Forecasting"><a href="#Forecasting" class="headerlink" title="Forecasting"></a>Forecasting</h4><ul>
<li>challenges<ul>
<li>Underfitting</li>
<li>Overfitting</li>
</ul>
</li>
<li>If we know the data and wait long enough, we will know the answer.</li>
</ul>
<h4 id="Causal-ï¼ˆfor-structuralï¼‰"><a href="#Causal-ï¼ˆfor-structuralï¼‰" class="headerlink" title="Causal ï¼ˆfor structuralï¼‰"></a>Causal ï¼ˆfor structuralï¼‰</h4><ul>
<li>Correlation: how two random variables move together </li>
<li>The difference between causation and correlation is a key concept in econometrics. We would like to identify causal effects and estimate their magnitude. </li>
<li>It is generally agreed that this is very difficult to do; having an economic model is often essential in establishing the causal interpretation.</li>
<li><strong>Unless run perfect experiment, we will never know the answer.</strong></li>
<li>requires $ğ¸(ğ‘¢|ğ‘¥) &#x3D; 0$</li>
<li><strong>Econometrics focus on causal problems inherent in collecting and analyzing observational economic data.</strong> </li>
<li>æœ‰å‡ ç§å¯èƒ½ï¼š<ul>
<li>x -&gt; y</li>
<li>z -&gt; x, z -&gt; y</li>
<li>y -&gt; x</li>
</ul>
</li>
<li>Example<ul>
<li><em>Mothers smoke one more cigarette during pregnancy causes their child to have 15g lower birth weight.</em></li>
</ul>
</li>
</ul>
<h3 id="Structure-of-Economic-data"><a href="#Structure-of-Economic-data" class="headerlink" title="Structure of Economic data"></a>Structure of Economic data</h3><h4 id="Cross-sectional-data"><a href="#Cross-sectional-data" class="headerlink" title="Cross-sectional data"></a>Cross-sectional data</h4><ul>
<li>A cross-sectional data set consists of a sample of units taken at a given point in time. </li>
<li>æˆªé¢æ•°æ®</li>
<li>assume:<ul>
<li>sample is drawn from the underlying population randomly</li>
<li>Violation of random sampling: We want to obtain a random sample of family income. However, wealthier families are more likely to refuse to report.</li>
</ul>
</li>
</ul>
<h4 id="Time-Series-data"><a href="#Time-Series-data" class="headerlink" title="Time-Series data"></a>Time-Series data</h4><ul>
<li>Each observation is uniquely determined by time </li>
<li>æ—¶é—´åºåˆ—</li>
<li>A time series data set consists of observations on a variable or several variables over time.<ul>
<li>å¯ä»¥æ˜¯å¯¹å¤šä¸ªå˜é‡çš„æ—¶é—´åºåˆ—</li>
</ul>
</li>
<li>Time is an important dimension in a time series data set.</li>
</ul>
<h4 id="Pooled-Cross-Sections-and-Panel-or-Longitudinal-Data"><a href="#Pooled-Cross-Sections-and-Panel-or-Longitudinal-Data" class="headerlink" title="Pooled Cross Sections and Panel or Longitudinal Data"></a>Pooled Cross Sections and Panel or Longitudinal Data</h4><ul>
<li>Pooled cross sections include cross-sectional data in multiple years. </li>
<li>A panel data set consists of a time series for each cross-sectional member in the data set.</li>
<li>Panel data: <ul>
<li>the same units over time</li>
</ul>
</li>
<li>pooled cross sections: <ul>
<li>diferent units, diferent time.</li>
</ul>
</li>
<li>Each observation is uniquely determined by the unit and the time.</li>
<li>åŒæ—¶å…·å¤‡æ—¶é—´å’Œå˜é‡å·®å¼‚</li>
</ul>
<hr>
<h2 id="Ch2-The-Simple-Regression-Model"><a href="#Ch2-The-Simple-Regression-Model" class="headerlink" title="Ch2 The Simple Regression Model:"></a>Ch2 The Simple Regression Model:</h2><h3 id="Interpretation-and-Estimation"><a href="#Interpretation-and-Estimation" class="headerlink" title="Interpretation and Estimation"></a>Interpretation and Estimation</h3><h4 id="Descriptive-analysis"><a href="#Descriptive-analysis" class="headerlink" title="Descriptive analysis"></a>Descriptive analysis</h4><ul>
<li>Define conditional expectation Eï¼ˆy|xï¼‰<ul>
<li>if I condition X to be some value, what is the expected value of Y?</li>
</ul>
</li>
</ul>
<h4 id="Simple-Linear-model"><a href="#Simple-Linear-model" class="headerlink" title="Simple Linear model"></a>Simple Linear model</h4><ul>
<li><p>$$E(y|x)&#x3D;\beta_0+\beta_1x$$</p>
</li>
<li><p>$$\beta_0&#x3D;E(y|x&#x3D;0)$$</p>
</li>
<li><p>$$\beta_1&#x3D;\frac{\partial E(y|x)}{\partial x}$$</p>
</li>
<li><p>let $u&#x3D;y-E(y|x)$, thus $E(u|x)&#x3D;0$</p>
</li>
<li><p>$$\hat{u_i}&#x3D;y_i-\hat{\beta_0}-\hat{\beta_1}x_i$$</p>
</li>
<li><p>using law of iterated expectation, we get</p>
<ul>
<li>$E(u)&#x3D;0$, and $E(ux)&#x3D;0$</li>
</ul>
</li>
<li><p>å¯¹äºæ€»ä½“ï¼Œå†™æˆ $y_i&#x3D;\beta_0+\beta_1x_i+u_i$</p>
</li>
<li><p>å¯¹äºæ ·æœ¬ï¼Œå†™æˆ $y_i&#x3D;\hat{\beta_0}+\hat{\beta_1}x_i+\hat{u_i}$</p>
<ul>
<li>å¸¦å¸½å­è¡¨ç¤ºçš„æ˜¯æ ·æœ¬ï¼Œç”¨æ¥ä¼°è®¡å®é™…å€¼</li>
</ul>
</li>
</ul>
<h5 id="Method-of-Moments"><a href="#Method-of-Moments" class="headerlink" title="Method of Moments"></a>Method of Moments</h5><ul>
<li><p>Method of moments: use the sample average to estimate the population expectation </p>
</li>
<li><p>Use $\frac{1}{N}\sum$ to replace $E[Â·]$</p>
<table>
<thead>
<tr>
<th>population expectations</th>
<th>sample analogue</th>
</tr>
</thead>
<tbody><tr>
<td>$E(u)&#x3D;0$</td>
<td>$\frac{1}{N}\sum \hat{u_i}&#x3D;0$</td>
</tr>
<tr>
<td>$E(ux)&#x3D;0$</td>
<td>$\frac{1}{N}\sum x_i\hat{u_i}&#x3D;0$</td>
</tr>
</tbody></table>
</li>
<li><p>using $\hat{u_i}&#x3D;y_i-\hat{\beta_0}-\hat{\beta_1}x_i$ to represent u, the result is :</p>
<ul>
<li>$$\hat{\beta_1}&#x3D;\frac{\frac{1}{N}\sum_{i&#x3D;1}^N(x_i-\bar{x})(y_i-\bar{y})}{\frac{1}{N}\sum_{i&#x3D;1}^N(x_i-\bar{x})^2}$$</li>
<li>$$\hat{\beta_0}&#x3D;\bar{y}-\hat{\beta_1}\bar{x}$$</li>
</ul>
</li>
</ul>
<h4 id="Causal-Estimation"><a href="#Causal-Estimation" class="headerlink" title="Causal Estimation"></a>Causal Estimation</h4><ul>
<li>$$y&#x3D;\beta_0+\beta_1 x+u$$<ul>
<li>$\beta_{0}$ and $\beta_1$ are unknown numbers in the nature we want to uncover</li>
<li>You choose x</li>
<li>Nature chose u in a way that is unrelated to your choice of x</li>
</ul>
</li>
<li>u represent things affect y other than x<ul>
<li>we think of u as some real thing, Itâ€™s just we canâ€™t observe it</li>
<li>u æ˜¯ä¸€ä¸ªå®é™…å­˜åœ¨çš„å˜é‡</li>
<li>To estimate the model,we need to know how u is determined.</li>
<li>The simplest case is that u is assigned at random </li>
<li>We can write this as $E(u|x)&#x3D;0$.<ul>
<li><strong>u ä¸éšç€ x è€Œå˜</strong><ul>
<li>é‡è¦ï¼åæ–¹å·®ä¸º0</li>
</ul>
</li>
<li>å‡å€¼æ˜¯0</li>
</ul>
</li>
</ul>
</li>
<li>å¯¹äºä¸Šå¼ä¸¤è¾¹å–æœŸæœ›ï¼Œå¯¹xæ±‚åå¯¼ï¼š<ul>
<li>$$\frac{\partial E[y|x]}{\partial x}&#x3D;\beta_{1}+\frac{\partial E[u|x]}{\partial x}$$</li>
<li>åªæœ‰å½“$E[u|x]$ æ˜¯å¸¸æ•°æ—¶,$\beta_1$ è¡¨ç¤ºå½“xå˜åŒ–1æ—¶ï¼Œyçš„å¹³å‡å˜åŒ–ã€‚</li>
<li>This condition gives the model a causal interpretation:<ul>
<li>if $E(u|x)$ does not vary when x changes, then any change in y can be attributed to x</li>
</ul>
</li>
<li>å› æ­¤$\beta_{1}$ ååº”äº†xå¯¹yçš„å› æœå…³ç³»</li>
</ul>
</li>
</ul>
<h4 id="Forecasting-1"><a href="#Forecasting-1" class="headerlink" title="Forecasting"></a>Forecasting</h4><ul>
<li>æˆ‘ä»¬å¸Œæœ›å¾—åˆ°ï¼š<ul>
<li>$$\hat{y}^*&#x3D;\hat{\beta_0}+\hat{\beta_1}x^*$$</li>
</ul>
</li>
<li>å’Œcausalä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰è‡ªå·±é€‰æ‹© $x^*$</li>
<li>ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ï¼Œargmaxæ¥åˆ¤æ–­å›å½’çš„yå’ŒçœŸå®å€¼çš„å…³ç³»ã€‚</li>
</ul>
<p><strong>ä¸¤ç§æ–¹æ³•ï¼šmomentå’ŒOLS</strong></p>
<h3 id="Properties-of-Simple-Regression-Model"><a href="#Properties-of-Simple-Regression-Model" class="headerlink" title="Properties of Simple Regression Model"></a>Properties of Simple Regression Model</h3><h4 id="Properties-of-OLS-on-Any-Sample-of-Data"><a href="#Properties-of-OLS-on-Any-Sample-of-Data" class="headerlink" title="Properties of OLS on Any Sample of Data"></a>Properties of OLS on Any Sample of Data</h4><ul>
<li><p>$$\sum_{i&#x3D;1}^{N}\hat{u}_{i}&#x3D;0.$$</p>
</li>
<li><p>$$\sum_{i&#x3D;1}^{N}x_{i}{\hat{u}}_{i}&#x3D;0.$$</p>
</li>
<li><p>$$\sum_{i&#x3D;1}^N\hat{y}_i\hat{u}_i&#x3D;0$$</p>
</li>
<li><p>$$\hat{\beta_0}+\hat{\beta_1}\bar{x}&#x3D;\bar{y}$$</p>
</li>
</ul>
<h4 id="Goodness-of-Fit"><a href="#Goodness-of-Fit" class="headerlink" title="Goodness of Fit"></a>Goodness of Fit</h4><ul>
<li>measure how well our model fits the data</li>
<li>decompose $y_i$ into two parts: the fitted value and the residual.<ul>
<li>$$y_i&#x3D;\hat{y_i}+\hat{u_i}$$</li>
<li>ç¬¬ä¸€éƒ¨åˆ†æ˜¯æ¨¡å‹è§£é‡Šçš„ï¼Œç¬¬äºŒéƒ¨åˆ†ä¸æ˜¯</li>
</ul>
</li>
<li>Define the following termsï¼š<ul>
<li><p>Total sum of squaresï¼ˆSSTï¼‰$$S S T&#x3D;\sum_{i&#x3D;1}^{N}\bigl(y_{i}-\bar{y}\bigr)^{2}.$$</p>
</li>
<li><p>Explained sum of squaresï¼ˆSSEï¼‰$$S S E&#x3D;\sum_{i&#x3D;1}^{N}({\hat{y}}_{i}-{\bar{y}})^{2}.$$</p>
</li>
<li><p>Residual sum of squaresï¼ˆSSRï¼‰$$S S R&#x3D;\sum_{i&#x3D;1}^{N}\hat{u}_{i}^{2}.$$</p>
</li>
<li><p>$$SST&#x3D;SSE+SSR$$</p>
</li>
</ul>
</li>
<li>å®šä¹‰ Goodness of fitï¼Œ $R^2$<ul>
<li>$$R^2&#x3D;\frac{SSE}{SST}$$</li>
<li>å³yæœ‰å¤šå¤§çš„éƒ¨åˆ†æ˜¯ç”±$y_i$ è§£é‡Šçš„</li>
<li>æ€»æ˜¯åœ¨0åˆ°1ä¹‹é—´</li>
<li>åªæ˜¯æè¿°äº†xå’Œyçš„ç›¸å…³æ€§ï¼Œå¹¶ä¸èƒ½è¡¨ç¤ºå› æœå…³ç³»</li>
</ul>
</li>
</ul>
<h4 id="Functional-form"><a href="#Functional-form" class="headerlink" title="Functional form"></a>Functional form</h4><ul>
<li><p>$$log(y)&#x3D;\beta_0+\beta_1 x+u$$</p>
<ul>
<li>$$\beta_1&#x3D;\frac{dlog(y)}{dy}&#x3D;\frac{dy}{y}\cdot\frac{1}{x}$$</li>
</ul>
</li>
<li><p><img src="https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230407231705.png" alt="Functional forms involving log"></p>
</li>
</ul>
<h4 id="Expected-Values-and-Variances-of-the-OLS-Estimators"><a href="#Expected-Values-and-Variances-of-the-OLS-Estimators" class="headerlink" title="Expected Values and Variances of the OLS Estimators"></a>Expected Values and Variances of the OLS Estimators</h4><h5 id="Unbiasedness-of-OLS"><a href="#Unbiasedness-of-OLS" class="headerlink" title="Unbiasedness of OLS"></a>Unbiasedness of OLS</h5><ul>
<li>Unbiasedness means the expectation of the estimator equals the true value.</li>
<li>æ— åæ€§</li>
<li>$E(\hat{\beta})&#x3D;\beta$</li>
</ul>
<h5 id="å‡è®¾ï¼šSLRï¼ˆsimple-linear-regressionï¼‰"><a href="#å‡è®¾ï¼šSLRï¼ˆsimple-linear-regressionï¼‰" class="headerlink" title="å‡è®¾ï¼šSLRï¼ˆsimple linear regressionï¼‰"></a>å‡è®¾ï¼šSLRï¼ˆsimple linear regressionï¼‰</h5><ol>
<li>Linear in Parameters </li>
<li>Random Sampling <ul>
<li>$Cov(u_i,u_j)&#x3D;0$</li>
</ul>
</li>
<li>Sample Variation in the Explanatory Variable <ul>
<li>å³è‡ªå˜é‡xçš„å–å€¼ä¸èƒ½åªæœ‰ä¸€ä¸ªç‚¹</li>
</ul>
</li>
<li>Zero Conditional Mean: $E(u|x) &#x3D; 0$</li>
</ol>
<p>åœ¨ä¸Šé¢å››ä¸ªæ¡ä»¶æˆç«‹æ—¶ï¼Œ$\beta_0$ å’Œ $\beta_1$ éƒ½æ»¡è¶³æ— åæ€§</p>
<ul>
<li>Though the OLS estimator is unbiased, it is still possible that the estimates calculated using the sample is very different from Î² in the population.</li>
</ul>
<ol start="5">
<li>Homoskedasticity <ul>
<li>The error u has the same variance given any value of the explanatory variable. </li>
<li>In other words, $Var(u|x)&#x3D;\sigma^2$</li>
<li>åŒæ–¹å·®æ€§</li>
<li><img src="https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230408134326.png" alt="left: homoskedasticity; right: heteroskedasticity"></li>
</ul>
</li>
</ol>
<p>åœ¨ä¸Šé¢5ä¸ªæ¡ä»¶æˆç«‹æ—¶ï¼Œå¯ä»¥æ±‚æ–¹å·®ï¼š</p>
<ul>
<li>$$Var(\hat{\beta_1}|x)&#x3D;\frac{\sigma^2}{\sum_{i&#x3D;1}^N(x_i-\bar{x})^2}&#x3D;\frac{\sigma^2}{SST_x}$$<ul>
<li>å…¶ä¸­ $\sigma$ æ˜¯ u çš„æ ‡å‡†å·®</li>
<li>å½“ $\sigma$ å¤§æ—¶$V a r(\hat{\beta}_{1}|x)$ æ–¹å·®å¤§</li>
<li>å½“ x çš„æ–¹å·®å¤§æ—¶ $V a r(\hat{\beta}_{1}|x)$ å°</li>
</ul>
</li>
<li>$$V a r(\hat{\beta_0}|x)&#x3D;\frac{\sigma^2\sum_{i&#x3D;1}^Nx_i^2}{N\sum_{i&#x3D;1}^N(x_i-\bar{x})^2}$$</li>
</ul>
<h5 id="ä¼°è®¡-sigma"><a href="#ä¼°è®¡-sigma" class="headerlink" title="ä¼°è®¡$\sigma$"></a>ä¼°è®¡$\sigma$</h5><ul>
<li>éœ€è¦ç”¨sampleæ¥ä¼°è®¡ $\sigma^2$</li>
<li>$$s^2\equiv\hat{\sigma}^2&#x3D;\frac{1}{N-2}\sum_{i&#x3D;1}^N\hat{u}_i^2$$</li>
<li>è¿™æ˜¯æ— åä¼°è®¡ï¼Œå…¶ä¸­-2æ˜¯å› ä¸ºæœ‰ä¸¤ä¸ªçº¦æŸæ¡ä»¶ï¼Œå°‘äº†ä¸¤ä¸ªè‡ªç”±åº¦ã€‚</li>
</ul>
<h2 id="Ch3-Multiple-Regression-Analysis-Estimation"><a href="#Ch3-Multiple-Regression-Analysis-Estimation" class="headerlink" title="Ch3 Multiple Regression Analysis: Estimation"></a>Ch3 Multiple Regression Analysis: Estimation</h2><h3 id="Why-we-need-multiple-regression-model"><a href="#Why-we-need-multiple-regression-model" class="headerlink" title="Why we need multiple regression model?"></a>Why we need multiple regression model?</h3><ul>
<li>Descriptive analysis: sometimes we want to estimate the conditional mean of y on multiple variables</li>
<li>Causal estimation: we know that something other than x may afect y, so we explicitly control them.</li>
<li>Forecasting: we want to use more variables to better predict y</li>
</ul>
<h3 id="Estimation-and-Interpretation"><a href="#Estimation-and-Interpretation" class="headerlink" title="Estimation and Interpretation"></a>Estimation and Interpretation</h3><ul>
<li>Population regression model:$$y&#x3D;\beta_0+\beta_1 x_1+\cdots+\beta_nx_n+u$$<ul>
<li>zero conditional mean:<ul>
<li>$$E(u|x_1,\cdots,x_n)&#x3D;0$$</li>
</ul>
</li>
<li>besides, using law of iterated expectation<ul>
<li>$$E(x_ju)&#x3D;0\quad E(u)&#x3D;0$$</li>
</ul>
</li>
</ul>
</li>
<li>Fitted value: $$\hat{y_i}&#x3D;\hat{\beta_0}+\hat{\beta_1}x_{i1}+\cdots+\hat{\beta_k}x_{i k}$$</li>
<li>residual: $$\hat{u_i}&#x3D; y_{i}-\hat{y_i}$$</li>
</ul>
<h4 id="Sample-analog"><a href="#Sample-analog" class="headerlink" title="Sample analog"></a>Sample analog</h4><table>
<thead>
<tr>
<th>population expectations</th>
<th>sample analogue</th>
</tr>
</thead>
<tbody><tr>
<td>$E(u)&#x3D;0$</td>
<td>$\frac{1}{N}\sum \hat{u_i}&#x3D;0$</td>
</tr>
<tr>
<td>$E(x_1u)&#x3D;0$</td>
<td>$\frac{1}{N}\sum x_{i1}\hat{u_i}&#x3D;0$</td>
</tr>
<tr>
<td>$E(x_ku)&#x3D;0$</td>
<td>$\frac{1}{N}\sum x_{ik}\hat{u_i}&#x3D;0$</td>
</tr>
</tbody></table>
<h4 id="OLS"><a href="#OLS" class="headerlink" title="OLS"></a>OLS</h4><ul>
<li>$$H\equiv\sum_{i&#x3D;1}^n\hat{u_i}^2&#x3D;\sum(y_i-b_0-b_1x_{i1}-\cdots-b_kx_{i k})^2$$</li>
<li>æ±‚æœ€å°å€¼ï¼Œå¾—ä¸€é˜¶æ¡ä»¶ï¼š<ul>
<li>$$\frac{\partial H}{\partial b_0}&#x3D;-\sum_{i&#x3D;1}^n2(y_i-\hat{\beta_0}-\hat{\beta_1}x_{i1}-\cdots-\hat{\beta_k}x_{ik})&#x3D;0$$</li>
<li>$${\frac{\partial H}{\partial b_{j}}}&#x3D;-\sum_{i&#x3D;1}^{n}2x_{i j}(y_{i}-{\hat{\beta_0}}-{\hat{\beta_1}}x_{i1}-\cdots-{\hat{\beta_k}}x_{i k})&#x3D;0,\forall j&#x3D;1,2,â€¦,k.$$</li>
</ul>
</li>
<li>OLS and sample analogue give the same answer.</li>
</ul>
<h4 id="Interpretation"><a href="#Interpretation" class="headerlink" title="Interpretation"></a>Interpretation</h4><ul>
<li><p>The coeicient of $x_i$ represents holding fixed other factors, the change in y when $x_i$ increases by one unit.</p>
</li>
<li><p>$$\Delta\hat{y}&#x3D;\hat{\beta}_1 \Delta x_1+\hat{\beta}_2 \Delta x_2+\cdots+\hat{\beta}_k \Delta x_k$$</p>
</li>
<li><p>æ±‚ $\hat{\beta_j}$: Frisch-Waugh-Lovell Theorem</p>
<ul>
<li>Regress $x_j$ on other independent variables ï¼ˆincluding the constantï¼‰, obtain the residual $\hat{r_{ij}}$.</li>
<li>Regress y on other independent variables ï¼ˆincluding the constantï¼‰, obtain the residual $\hat{r_{iy}}$.</li>
<li>Regress $\hat{r_{iy}}$ on $\hat{r_{ij}}$, The resulting slope coefficient is $\hat{\beta_j}$</li>
</ul>
</li>
</ul>
<h4 id="Goodness-of-fit"><a href="#Goodness-of-fit" class="headerlink" title="Goodness of fit"></a>Goodness of fit</h4><ul>
<li>å’Œ SLR ç›¸åŒ</li>
<li>ä½†éšç€å˜é‡å¢åŠ  $R^2$ å‡ ä¹ä¸€å®šä¼šå¢å¤§</li>
<li>å¼•å…¥ Adjusted R2<ul>
<li>$$\bar{R}^2&#x3D;1-\frac{SSR&#x2F;(N-k-1)}{SST&#x2F;N-1}&#x3D;1-(1-R^2)\frac{N-1}{N-k-1}$$</li>
<li>N represents the size of the sample, k represents the number of independent variables ï¼ˆexcluding the constantï¼‰</li>
</ul>
</li>
</ul>
<h3 id="Expected-Values-and-Variances-of-the-OLS-Estimators-1"><a href="#Expected-Values-and-Variances-of-the-OLS-Estimators-1" class="headerlink" title="Expected Values and Variances of the OLS Estimators"></a>Expected Values and Variances of the OLS Estimators</h3><h4 id="å‡è®¾ï¼šMLRï¼ˆmultiple-linear-regressionï¼‰"><a href="#å‡è®¾ï¼šMLRï¼ˆmultiple-linear-regressionï¼‰" class="headerlink" title="å‡è®¾ï¼šMLRï¼ˆmultiple linear regressionï¼‰"></a>å‡è®¾ï¼šMLRï¼ˆmultiple linear regressionï¼‰</h4><ol>
<li>Linear in Parameters </li>
<li>Random Sampling <ul>
<li>$Cov(u_i,u_j)&#x3D;0$</li>
</ul>
</li>
<li>No perfect collinearity<ul>
<li>ä¸èƒ½çº¿æ€§ç›¸å…³</li>
<li>ä¸ç„¶æ— æ³•åŒºåˆ«è¿™äº›çº¿æ€§ç›¸å…³æˆåˆ†</li>
</ul>
</li>
<li>Zero Conditional Mean: <ul>
<li>$E(u|x_1,\cdots,x_k) &#x3D; 0$</li>
</ul>
</li>
</ol>
<ul>
<li>åœ¨ä¸Šé¢å››ä¸ªæ¡ä»¶æˆç«‹æ—¶ï¼Œ$\beta_0$ å’Œ $\beta_1$ éƒ½æ»¡è¶³æ— åæ€§</li>
</ul>
<ol start="5">
<li>Homoskedasticity <ul>
<li>$Var(u|x_1,\cdots,x_k)&#x3D;\sigma^2$</li>
<li>åŒæ–¹å·®æ€§</li>
</ul>
</li>
</ol>
<ul>
<li><p>MLR1-5åˆæˆ Gauss-Markov Assumption</p>
</li>
<li><p>åœ¨Gauss-Markovæ¡ä»¶ä¸‹ï¼Œæ–¹å·®æ»¡è¶³ï¼š</p>
<ul>
<li>$$V a r(\hat{\beta_j})&#x3D;\frac{\sigma^{2}}{SST_j(1-R_j^2)}$$</li>
<li>$R_j^2$ is the R-squared from regressing $x_j$ on all other independent variables and including an intercept</li>
</ul>
</li>
<li><p>The unbiased estimator of $Ïƒ^2$ is:</p>
<ul>
<li>$$\hat{\sigma}^{2}&#x3D;\frac{1}{N-k-1}\sum_{i&#x3D;1}^{n}\hat{u}_{i}^{2}.$$</li>
<li>è‡ªç”±åº¦ï¼šN-k-1</li>
<li>æ˜¯æ— åä¼°è®¡</li>
</ul>
</li>
</ul>
<h4 id="BLUE"><a href="#BLUE" class="headerlink" title="BLUE"></a>BLUE</h4><table>
<thead>
<tr>
<th>sdandard deviation</th>
<th>standard error</th>
</tr>
</thead>
<tbody><tr>
<td>$sd(\hat{\beta_j})$</td>
<td>$se(\hat{\beta_j})$</td>
</tr>
<tr>
<td>$\frac{\sigma}{[SST_j(1-R_j^2)]^{1&#x2F;2}}$</td>
<td>$\frac{\hat{\sigma}}{[SST_j(1-R_j^2)]^{1&#x2F;2}}$</td>
</tr>
<tr>
<td>$\sigma^2&#x3D;\text{Var of u}$</td>
<td>$\hat{\sigma}^{2}&#x3D;\frac{1}{n-k-1}\sum \hat{u}^2$</td>
</tr>
<tr>
<td>unknown</td>
<td>estimated using sample</td>
</tr>
</tbody></table>
<ul>
<li>OLS æ˜¯ best linear unbiased estimator, BLUE<ul>
<li>æ»¡è¶³çº¿æ€§ï¼Œå¹¶ä¸”æ–¹å·®æœ€å°ã€‚</li>
</ul>
</li>
</ul>
<h3 id="Practical-issues"><a href="#Practical-issues" class="headerlink" title="Practical issues"></a>Practical issues</h3><h4 id="Omitted-bias"><a href="#Omitted-bias" class="headerlink" title="Omitted bias"></a>Omitted bias</h4><ul>
<li><p>å‡è®¾æœ‰ä¸¤ä¸ªè‡ªå˜é‡ï¼Œä½†åªå¯¹å…¶ä¸­ä¸€ä¸ªè¿›è¡Œå›å½’ï¼Œé‚£ä¹ˆå¾—åˆ°çš„$\hat{\beta}$ ä¸å®é™…å€¼æœ‰ä¸€ä¸ªè¯¯å·®ã€‚</p>
</li>
<li><p>$E(\tilde{\beta}_1)&#x3D;\beta_1+\beta_2\tilde{\delta}_1$</p>
</li>
<li><p>thus, $Bias(\tilde{\beta_1})&#x3D;\beta_{2}\tilde{\delta}_{1}$</p>
</li>
<li><table>
<thead>
<tr>
<th></th>
<th>$Corr(x_1,x_2)&gt;0$</th>
<th>$Corr(x_1,x_2)&lt;0$</th>
</tr>
</thead>
<tbody><tr>
<td>$\beta_2&gt;0$</td>
<td>Positive Bias</td>
<td>Negative Bias</td>
</tr>
<tr>
<td>$\beta_2&lt;0$</td>
<td>Negative Bias</td>
<td>Positive Bias</td>
</tr>
</tbody></table>
</li>
<li><p>å½±å“æ— åæ€§</p>
</li>
</ul>
<h4 id="including-irrelavent"><a href="#including-irrelavent" class="headerlink" title="including irrelavent"></a>including irrelavent</h4><ul>
<li>ä¸å½±å“æ— åæ€§ï¼Œä½†æ–¹å·®ä¼šå˜å¤§</li>
</ul>
<h4 id="Multicollinearity"><a href="#Multicollinearity" class="headerlink" title="Multicollinearity"></a>Multicollinearity</h4><ul>
<li>high ï¼ˆbut not perfectï¼‰ correlation between two or more independent variables</li>
<li>ä¸å½±å“æ— åæ€§ï¼Œä½†æ–¹å·®ä¼šå˜å¤§</li>
</ul>
<h2 id="Ch4-Multiple-Regression-Analysis-Inference"><a href="#Ch4-Multiple-Regression-Analysis-Inference" class="headerlink" title="Ch4 Multiple Regression Analysis: Inference"></a>Ch4 Multiple Regression Analysis: Inference</h2><h3 id="Classical-Linear-Regression-Model"><a href="#Classical-Linear-Regression-Model" class="headerlink" title="Classical Linear Regression Model"></a>Classical Linear Regression Model</h3><h4 id="The-Distribution-of-hat-Î²-j"><a href="#The-Distribution-of-hat-Î²-j" class="headerlink" title="The Distribution of $\hat{Î²_j}$"></a>The Distribution of $\hat{Î²_j}$</h4><ul>
<li><p>ä¸ºäº†å¾—åˆ°åˆ†å¸ƒï¼Œåœ¨Gauss-Markovæ¡ä»¶ä¸Šè¿˜è¦åŠ ä¸Šä¸€æ¡ï¼š</p>
</li>
<li><p>MLR6: Normality</p>
<ul>
<li>$$u\sim N(0,\sigma^2)$$</li>
</ul>
</li>
<li><p>Assumptions MLR.1 through MLR.6 are called the classical linear model <strong>CLM</strong> assumptions. </p>
</li>
<li><p>summarize the population assumptions of the CLM is</p>
<ul>
<li>$$y|{\bf x}\ \sim\ No r m a l(\beta_{0}\ +\beta_{1}x_{1}+\ldots+\beta_{k}x_{k},\sigma^{2})$$</li>
</ul>
</li>
<li><p>åœ¨CLMæ¡ä»¶ä¸‹ï¼Œ$\beta$ æœä»æ­£æ€åˆ†å¸ƒ</p>
<ul>
<li>$$\frac{\hat{\beta}_j-\beta_j}{sd(\hat{\beta}_j)}\sim Normal(0,1)$$</li>
</ul>
</li>
<li><p>ä½†ç”±äºå®é™…ä¸­æ ‡å‡†å·®ä¸çŸ¥é“ï¼Œéœ€è¦ç”¨æ ‡å‡†è¯¯æ¥ç®—ã€‚</p>
<ul>
<li>$$\frac{\hat{\beta_j}-\beta_j}{s e(\hat{\beta_j})}\sim t_{N-k-1}&#x3D; t_{d f},$$</li>
<li>æœä»tåˆ†å¸ƒï¼ŒN-k-1æ˜¯è‡ªç”±åº¦ï¼ŒNæ˜¯å¤šå°‘ä¸ªæ ·æœ¬ï¼Œkæ˜¯å›å½’é‡Œé¢æœ‰å¤šå°‘ä¸ªx</li>
</ul>
</li>
</ul>
<h3 id="tæ£€éªŒ"><a href="#tæ£€éªŒ" class="headerlink" title="tæ£€éªŒ"></a>tæ£€éªŒ</h3><ul>
<li>æ£€éªŒæŸä¸ªç³»æ•°æ˜¯å¦ä¸º0</li>
</ul>
<h4 id="Null-hypothesis"><a href="#Null-hypothesis" class="headerlink" title="Null hypothesis"></a>Null hypothesis</h4><ul>
<li>Let $H_0$ be the null hypothesis that we want to test. Let $H_1$ be the alternative hypothesis. </li>
<li>We reject the null hypothesis when the test statistic falls in the rejection region.</li>
</ul>
<h4 id="rejection-region"><a href="#rejection-region" class="headerlink" title="rejection region"></a>rejection region</h4><ul>
<li>type 1 error:<ul>
<li>significance level &#x3D; Î± &#x3D; $Pr(\text{rejecting }H_0|H_0\text{ is true})$</li>
</ul>
</li>
<li>type 2 error:<ul>
<li>$Pr(\text{not rejecting }H_0|H_1\text{ is true})$</li>
</ul>
</li>
<li>æˆ‘ä»¬çš„æƒ³æ³•æ˜¯å…ˆå›ºå®šä¸€ä¸ªsignificance levelï¼Œç¡®å®šå¯¹type 1 error çš„å®¹å¿åº¦ï¼Œç„¶åå†æœ€å°åŒ– type 2 error</li>
</ul>
<h4 id="Testing-Against-One-Sided-Alternatives"><a href="#Testing-Against-One-Sided-Alternatives" class="headerlink" title="Testing Against One-Sided Alternatives"></a>Testing Against One-Sided Alternatives</h4><ul>
<li>Suppose we are interested in testing<br>$$<br>\begin{array}{ll}<br>H_0: &amp; \beta_j&#x3D;0 . \\<br>H_1: &amp; \beta_j&gt;0 .<br>\end{array}<br>$$</li>
<li>Consider a test statistic:$$\frac{\hat{\beta}_j-\beta_j}{\operatorname{se(}(\hat{\beta}_j)}$$</li>
<li>When $H_0$ is true, the test statistic is: $$\frac{\hat{\beta_j}}{s e(\hat{\beta_j})} \sim t_{N-k-1}&#x3D;t_{df}$$</li>
</ul>
<ol>
<li>It depends on the data.</li>
<li>We know its distribution under $H_0$.</li>
</ol>
<ul>
<li>ä»¤ $$t_{\hat{\beta}_j} \equiv \frac{\hat{\beta}_j}{s e(\hat{\beta}_j)}$$</li>
<li>We often call $t_{\hat{\beta}_j}$ t-statistic or t-ratio of $\hat{\beta}_j$.</li>
<li>$t_{\hat{\beta}_j}$ has the same sign as $\hat{\beta}_j$, because $\operatorname{se}(\hat{\beta}_j)&gt;0$.</li>
</ul>
<ul>
<li>ç›´è§‰ä¸Š, å½“ $t_\hat{\beta_j}$ è¶³å¤Ÿå¤§çš„æ—¶å€™æ‹’ç» $H_0$ :  $t_\hat{\beta_i}$ è¶Šå¤§,  $H_0$ æ˜¯çœŸçš„å¯èƒ½æ€§è¶Šä½,  $H_1$ æ˜¯çœŸçš„å¯èƒ½æ€§è¶Šé«˜ã€‚</li>
</ul>
<ul>
<li>å¤šå¤§ç®—è¶³å¤Ÿå¤§ï¼Ÿ<ul>
<li>Fix a significance level of $5 %$. The critical value, $c$ is the 95 th percentile when $H_0$ is true. It means when $H_0$ is true, the probability of getting a value as large as $c$ is $5 %$.</li>
<li>Rejection rule:$t_{\hat{\beta}_j}&gt;c$</li>
</ul>
</li>
<li>Rejecting $H_0$ when $t_{\hat{\beta}_j}&gt;c$ means the probability of making a type I error, that is, the probability of rejecting $H_0$ when $H_0$ is true, is $5 %$.</li>
<li><img src="https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230408165619.png" alt="å•è¾¹æ£€éªŒï¼Œ5%"></li>
</ul>
<h5 id="The-idea-of-test"><a href="#The-idea-of-test" class="headerlink" title="The idea of test"></a>The idea of test</h5><ol>
<li>Fix a significance level $\alpha$. That is, decide our level of â€œtolerenceâ€ for the type I error.</li>
<li>Find the critical value associated with $\alpha$. For $H_1: \beta_j&gt;0$, this means finding the $(1-\alpha)$-th percentile of the $\mathrm{t}$ distribution with $d f&#x3D;N-k-1$.</li>
<li>Reject $H_0$ if  $t_{\hat{\beta}_j}&gt;c$</li>
</ol>
<ul>
<li>é€šå¸¸ç¬¬ä¸€ç±»é”™è¯¯å’Œç¬¬äºŒç±»é”™è¯¯æ˜¯ä¸èƒ½åŒæ—¶ç¼©å°çš„ã€‚éœ€è¦å–èˆã€‚</li>
</ul>
<h4 id="Two-sided-Alternatives"><a href="#Two-sided-Alternatives" class="headerlink" title="Two-sided Alternatives"></a>Two-sided Alternatives</h4><ul>
<li><p>We want to test:$$\begin{array}{ll}H_0: &amp; \beta_j&#x3D;0 . \\ H_1: &amp; \beta_j \neq 0 .<br>\end{array}$$</p>
</li>
<li><p>This is the relevant alternative when the sign of $\beta_j$ is not well determined by theory.</p>
</li>
<li><p>Even when we know whether $\beta_j$ is positive or negative under the alternative, a two-sided test is often prudent.</p>
</li>
<li><p>æ±‚æ³•ï¼š</p>
<ol>
<li>Fix a significance level $\alpha$. That is, decide our level of â€œtolerenceâ€ for the type I error.</li>
<li>Find the critical value associated with $\alpha$. For $H_1: \beta_j \neq 0$, this means finding the $(1-\alpha &#x2F; 2)$-th percentile of the $\mathrm{t}$ distribution with $d f&#x3D;N-k-1$.</li>
<li>Reject $H_0$ if$$|t_{\hat{\beta}_j}|&gt;c .$$</li>
</ol>
</li>
<li><p><img src="https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230408165324.png" alt="åŒè¾¹æ£€éªŒï¼Œ5%"></p>
</li>
<li><p>æ²¡æœ‰è¯´æ˜çš„è¯é€šå¸¸æ˜¯åŒè¾¹çš„</p>
</li>
<li><p>If $H_0$ is rejected in favor of $H_1: \quad \beta_j \neq 0$ at the $5 %$ level, we usually say that â€œ $x_j$ is statistically significant, or statistically different from zero, at the $5 %$ level.â€</p>
</li>
<li><p>If $H_0$ is not rejected, we say that â€œ $x_j$ is statistically insignificant at the $5 %$ level.â€</p>
</li>
</ul>
<h4 id="Other-Hypothesis"><a href="#Other-Hypothesis" class="headerlink" title="Other Hypothesis"></a>Other Hypothesis</h4><ul>
<li>If the null is stated as:<br>$$<br>H_0: \beta_j&#x3D;a_j<br>$$<br>Then the t-statistic is$$\frac{\hat{\beta_j}-a_j}{\operatorname{se}(\hat{\beta_j})} \sim t_{N-k-1}$$<br>We can use the general t statistic to test against one-sided or two-sided alternatives.</li>
</ul>
<h4 id="p-Values-for-t-Tests"><a href="#p-Values-for-t-Tests" class="headerlink" title="p-Values for t Tests"></a>p-Values for t Tests</h4><ul>
<li>Given the observed value of the t statistic, what is the smallest significance level at which the null hypothesis would be rejected? </li>
<li>We call this â€œsmallest signiicance levelâ€ p-value. </li>
<li>p-value represents the probability of observing a value as extreme as $t_{\hat{\beta}_j}$ under the $H_0$</li>
<li>$$<br>\begin{array}{ll}<br>H_0: &amp; \beta_j&#x3D;0 . \\<br>H_1: &amp; \beta_j \neq 0 .<br>\end{array}<br>$$<ul>
<li>The p-value in this case is$$P(|T|&gt;|t|),$$</li>
<li>where we let $T$ denote a $\mathrm{t}$ distributed random variable with $N-k-1$ degrees of freedom and let $t$ denote the numerical value of the test statistic.</li>
</ul>
</li>
</ul>
<ul>
<li>The p-value nicely summarizes the strength or weakness of the empirical evidence against the null hypothesis.</li>
<li>The p-value is the probability of observing a $t$ statistic as extreme as we did if the null hypothesis is true.</li>
<li>Signiicance level and critical valueä¸€ä¸€å¯¹åº”</li>
</ul>
<h4 id="Economic-versus-Statistical-Signiicance"><a href="#Economic-versus-Statistical-Signiicance" class="headerlink" title="Economic versus Statistical Signiicance"></a>Economic versus Statistical Signiicance</h4><ul>
<li>The statistical significance of a variable $x_j$ is determined entirely by the size of $t_{\hat{\beta}_j}$, whereas the economic significance or practical significance of a variable is related to the size (and sign) of $\hat{\beta}_j$.</li>
</ul>
<ul>
<li>We often care about both statistical significance and economic significance.</li>
</ul>
<h3 id="Confidence-interval"><a href="#Confidence-interval" class="headerlink" title="Confidence interval"></a>Confidence interval</h3><ul>
<li>We can construct a confidence level depending on $\alpha$. We call it a $(1-\alpha)$ confidence interval:$$[\hat{\beta}_j-c \cdot \operatorname{se}(\hat{\beta}_j), \hat{\beta}_j+c \cdot \operatorname{se}(\hat{\beta}_j)]$$</li>
<li>The critical value $c$ is the $(1-\alpha &#x2F; 2)$ percentile in a $\mathrm{t}$ distribution with $d f&#x3D;N-k-1$.</li>
<li>The meaning of a 95% conidence interval: if we sample repeatedly many times, then the true $Î²_j$ will appear in 95% of the confidence intervals. </li>
<li>å¯¹äºå¾ˆå¤šæ¬¡å–æ ·æ¥è¯´çš„å¯èƒ½æ€§ï¼Œä½†å¯¹äºæŸä¸€æ¬¡ç‰¹å®šçš„å–æ ·ä¸èƒ½ç¡®å®šæ˜¯å¦ä¸€å®šåœ¨ç½®ä¿¡åŒºé—´é‡Œé¢ã€‚</li>
</ul>
<p><strong>ä¸‰ç§æ–¹æ³•ä¸€æ ·ï¼š</strong></p>
<ol>
<li>Fix a significance level $\alpha$, calculate the critical value $c$, and then reject $H_0$ if $|t_{\hat{\beta}_j}|&gt;c$.</li>
<li>Fix a significance level $\alpha$, calculate the $\mathrm{p}$-value, reject $H_0$ if $p&lt;\alpha$.</li>
<li>reject if 0 is not in the confidence level.</li>
</ol>
<h3 id="Testing-Multiple-Linear-Restrictions-The-F-Test"><a href="#Testing-Multiple-Linear-Restrictions-The-F-Test" class="headerlink" title="Testing Multiple Linear Restrictions: The F Test"></a>Testing Multiple Linear Restrictions: The F Test</h3><ul>
<li>$$y&#x3D;\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_3 x_3+u$$We want to test$$H_0: \quad \beta_1&#x3D;0 \text { and } \beta_2&#x3D;0$$$$H_1: H_0\text{ is not true.}$$ </li>
<li>Method:<ul>
<li>Consider the restricted model when $H_0$ is true$$y&#x3D;\gamma_0+\gamma_3 x_3+u$$</li>
<li>If $H_0$ is true, the two models are the same. That means when we include $x_1$ and $x_2$ into the model, the sum of squared residuals should not change much.</li>
<li>However, if $H_0$ is false that means that at least one of $\beta_1, \beta_2$ is nonzero and the sum of squared residuals should fall when we include these new variables</li>
<li>çœ‹SSRæ˜¯å¦ç›¸ç­‰</li>
</ul>
</li>
</ul>
<ul>
<li>F-test<ul>
<li>$$F \equiv \frac{(S S R_r-S S R_{u r}) &#x2F; q}{S S R_{u r} &#x2F;(N-k-1)}&#x3D; \frac{(R_{u r}^2-R_r^2) &#x2F; q}{(1-R_{u r}^2) &#x2F;(N-k-1)}$$</li>
<li>$q$ is the number of linear restrictions, which is the difference in degrees of freedom in the restricted model versus the unrestricted model.</li>
<li>$S S R_r$ is the sum of squared residuals from the restricted model and $S S R_{u r}$ is the sum of squared residuals from the unrestricted model.</li>
<li>Since $S S R_r$ can be no smaller than $S S R_{u r}$, the $\mathrm{F}$ statistic is always nonnegative.</li>
<li>We can show that the sampling distribution of the F-stat: $F \sim F_{q, N-k-1}$. We call this an $\mathrm{F}$ distribution with $q$ degrees of freedom in the numerator and $N-k-1$ degrees of freedom in the denominator.</li>
<li><img src="https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230408191204.png" alt="F-test 5%"></li>
</ul>
</li>
<li>tåˆ†å¸ƒçš„å¹³æ–¹å°±æ˜¯Fåˆ†å¸ƒï¼Œå› æ­¤å•å˜é‡ä¹Ÿå¯ä»¥ç”¨Fåˆ†å¸ƒã€‚</li>
<li>In the $\mathrm{F}$ testing context, the $\mathrm{p}$-value is defined as$$P(\mathcal{F}&gt;F) $$where $\mathcal{F}$ denote an $\mathrm{F}$ random variable with $(q, N-k-1)$ degrees of freedom, and $\mathrm{F}$ is the actual value of the test statistic.</li>
<li>p-value is the probability of observing a value of $\mathrm{F}$ at least as large as we did, given that the null hypothesis is true.<ul>
<li>$\rightarrow$ Reject $H_0$ if $p&lt;\alpha$</li>
</ul>
</li>
</ul>
<ul>
<li>é€šå¸¸æƒ…å†µä¸‹ä½¿ç”¨SSRå½¢å¼çš„Fæ£€éªŒæ¯”è¾ƒå¥½ï¼Œå› ä¸ºæœ‰æ—¶å€™restrictedçš„å½¢å¼ä¸unrestrictedä¸åŒï¼Œä¸èƒ½ç›´æ¥ç”¨R2æ¥è®¡ç®—ã€‚</li>
</ul>
<h3 id="Testing-Multiple-Linear-Restrictions-The-LM-statistic"><a href="#Testing-Multiple-Linear-Restrictions-The-LM-statistic" class="headerlink" title="Testing Multiple Linear Restrictions: The LM statistic"></a>Testing Multiple Linear Restrictions: The LM statistic</h3><ul>
<li>Lagrange multiplier ï¼ˆLMï¼‰ statistic</li>
<li>The LM statistic can be used in testing multiple exclusion restrictions ï¼ˆas in an $F$ testï¼‰ under large sample.</li>
<li>å¯¹äºæ¨¡å‹$$y&#x3D;\beta_0+\beta_1 x_1+\ldots+\beta_k x_k+u$$We want to test whether the last $q$ of these variables all have zero population parameters:$$H_0: \beta_{k-q+1}&#x3D;\beta_{k-q+2}&#x3D;\ldots&#x3D;\beta_k&#x3D;0$$</li>
</ul>
<ul>
<li>$L M$ statistic</li>
<li>First estimate the restricted model:$$y&#x3D;\tilde{\beta_0}+\tilde{\beta_1} x_1+\cdots+\tilde{\beta_{k-q}} x_{k-q}+\tilde{u}$$If the coefficients of the excluded independent variables $x_{k-q+1}$ to $x_k$ are truly zero in the population model, then they should be uncorrelated to $\tilde{u}$.</li>
<li>So regress $\tilde{u}$ on all $x$$$\tilde{u} \sim x_1, x_2, \ldots, x_k$$Let $R_u^2$ denote the R-squared of this regression. The smaller the $R_u^2$, the more likely $H_0$ is true. So a large $R_u^2$ provides evidence against $H_0$.</li>
<li>$L M&#x3D;N \cdot R_u^2$. We can show that $L M$ follows chi-square distribution with $q$ degrees of freedom: $\mathcal{X}_q^2$.<ul>
<li>Reject $H_0$ if $L M&gt;$ critical value $(p&lt;$ significance level)</li>
<li><img src="https://cdn.mathpix.com/snip/images/X3leSxpzRqmmLAR3CZWjQs6eNsBj0o1ySEyLiIXgqNI.original.fullsize.png" alt="chi-square"></li>
</ul>
</li>
<li>åœ¨å¤§æ ·æœ¬æ—¶ï¼ŒLMå’ŒFæ£€éªŒç»“æœå¾ˆç›¸ä¼¼</li>
</ul>
<h2 id="Ch5-Multiple-Regression-Analysis-OLS-Asymptotics"><a href="#Ch5-Multiple-Regression-Analysis-OLS-Asymptotics" class="headerlink" title="Ch5 Multiple Regression Analysis: OLS Asymptotics"></a>Ch5 Multiple Regression Analysis: OLS Asymptotics</h2><h3 id="Asymptotic-Properties"><a href="#Asymptotic-Properties" class="headerlink" title="Asymptotic Properties"></a>Asymptotic Properties</h3><ul>
<li>Finite sample properties: properties hold for any sample of data.</li>
<li>Examples<ul>
<li>Unbiasedness of OLS</li>
<li>OLS is BLUE</li>
<li>Sampling distribution of the OLS estimators</li>
</ul>
</li>
<li>Asymptotic properties or large sample properties: not defined for a particular sample size; rather, they are defined as the sample size grows without bound.<ul>
<li>æ¸è¿‘æ€§è´¨</li>
</ul>
</li>
</ul>
<h3 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h3><ul>
<li>ä¸€è‡´æ€§</li>
<li>Let $W_N$ be an estimator of $\theta$ based on a sample $Y_1, Y_2, \ldots, Y_N$ of size $N$. Then, $W_N$ is a consistent estimator of $\theta$ if for every $\epsilon&gt;0$$$P(|W_N-\theta|&gt;\epsilon) \rightarrow 0 \text { as } N \rightarrow \infty .$$</li>
<li>We also say consistency means:$$\operatorname{plim}(W_N)&#x3D;\theta$$<ul>
<li>Intuitively, consistency means when the sample size becomes larger, the estimator gets closer and closer to the true value.</li>
</ul>
</li>
<li><strong>ä¸€è‡´æ€§å’Œæ— åæ€§æ²¡æœ‰å¿…ç„¶è”ç³»</strong></li>
</ul>
<h4 id="Consistency-of-OLS"><a href="#Consistency-of-OLS" class="headerlink" title="Consistency of OLS"></a>Consistency of OLS</h4><ul>
<li>Under Assumptions MLR.1 through MLR.4, the OLS estimator $\hat{\beta}_j$ is consistent for $\beta_j$, for all $j&#x3D;0,1, \ldots, k$.</li>
</ul>
<ul>
<li>When the sample size is larger, the OLS estimator is centered around the true parameter closer and closer.</li>
</ul>
<h4 id="Central-Limit-Theorem"><a href="#Central-Limit-Theorem" class="headerlink" title="Central Limit Theorem"></a>Central Limit Theorem</h4><ul>
<li>Use the notation$$\hat{\theta}_N \stackrel{a}{\sim} N(0, \sigma^2)$$to mean that as the sample size $N$ gets larger, $\hat{\theta}_N$ is approximately normally distributed with mean 0 and variance $\sigma^2$.</li>
<li><strong>Central Limit Theorem</strong><ul>
<li>Let ${Y_1, Y_2, \ldots, Y_N}$ be a random sample with mean $\mu$ and variance $\sigma^2$. Then,$$Z_N&#x3D;\frac{\bar{Y}_N-\mu}{\sigma &#x2F; \sqrt{N}} \stackrel{a}{\sim} N(0,1)$$</li>
<li>Intuitively, it means when the sample size gets larger, the distribution of the sample average is closer to a normal distribution.</li>
<li>ä¸ç®¡Yçš„åˆ†å¸ƒå¦‚ä½•ï¼Œå½“æ ·æœ¬é‡è¶³å¤Ÿå¤§ï¼Œéƒ½ä¼šè¶‹å‘äºæ­£æ€åˆ†å¸ƒ</li>
</ul>
</li>
</ul>
<h3 id="Asymptotic-Normality-of-OLS"><a href="#Asymptotic-Normality-of-OLS" class="headerlink" title="Asymptotic Normality of OLS"></a>Asymptotic Normality of OLS</h3><ul>
<li>Under the Gauss-Markov Assumptions MLR.1 through MLR. 5 , for each $j&#x3D;0,1, \ldots, k$</li>
<li>$$\begin{aligned}<br>&amp; \frac{\hat{\beta}_j-\beta_j}{s d(\hat{\beta}_j)} \stackrel{a}{\sim} \operatorname{Normal}(0,1) . \\<br>&amp; \frac{\hat{\beta}_j-\beta_j}{\operatorname{se}(\hat{\beta}_j)} \stackrel{a}{\sim} \operatorname{Normal}(0,1) .<br>\end{aligned}$$</li>
<li>OLS estimators are approximately normally distributed in large enough sample sizes.</li>
<li>è¿™ä¸ªå®šç†è¯´æ˜å½“æ ·æœ¬é‡è¶³å¤Ÿå¤§çš„æ—¶å€™ï¼Œä¸éœ€è¦uçš„æ­£æ€åˆ†å¸ƒå‡å®šã€‚</li>
</ul>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul>
<li>Under MLR.1-MLR.4, OLS estimators are consistent. </li>
<li>Under MLR.1-MLR.5, OLS estimators have an asymptotic normal distribution.</li>
</ul>
<h2 id="Ch6-Multiple-Regression-Analysis-Further-Issues"><a href="#Ch6-Multiple-Regression-Analysis-Further-Issues" class="headerlink" title="Ch6 Multiple Regression Analysis: Further Issues"></a>Ch6 Multiple Regression Analysis: Further Issues</h2><h3 id="Efects-of-Data-Scaling-on-OLS-Statistics"><a href="#Efects-of-Data-Scaling-on-OLS-Statistics" class="headerlink" title="Efects of Data Scaling on OLS Statistics"></a>Efects of Data Scaling on OLS Statistics</h3><h4 id="changing-unit-of-measurement"><a href="#changing-unit-of-measurement" class="headerlink" title="changing unit of measurement"></a>changing unit of measurement</h4><ul>
<li>Consider the simple regression model:$$y&#x3D;\beta_0+\beta_1 x+u$$</li>
<li>Now suppose $y^*&#x3D;w_1 y$ and $x^*&#x3D;w_2 x$, Then for this model:$$y^*&#x3D;\beta_0^*+\beta_1^* x^*+u$$</li>
<li>$\hat{\beta}_0^*$ and $\hat{\beta}_0$, and $\hat{\beta}_1^*$ and $\hat{\beta}_1$ çš„å…³ç³»æ˜¯ï¼Ÿ<ul>
<li>$$\hat{\beta}_0^*&#x3D;w_1 \hat{\beta}_0, \quad \hat{\beta}_1^*&#x3D;\frac{w_1}{w_2} \hat{\beta}_1$$</li>
<li>$$\operatorname{se}(\hat{\beta}_0^*) &#x3D;w_1 \operatorname{se}(\hat{\beta}_0), \quad\operatorname{se}(\hat{\beta}_1^*) &#x3D;\frac{w_1}{w_2}\operatorname{se}(\hat{\beta}_1)$$</li>
<li>$$t_\hat{\beta_0}^*&#x3D;t_\hat{\beta_0} \quad t_\hat{\beta_1}^* &#x3D;t_\hat{\beta_1}$$</li>
<li>$$R^{2*}&#x3D;R^2$$</li>
<li>the statistical signiicance does not change.</li>
</ul>
</li>
</ul>
<h4 id="Unit-Change-in-Logarithmic-Form"><a href="#Unit-Change-in-Logarithmic-Form" class="headerlink" title="Unit Change in Logarithmic Form"></a>Unit Change in Logarithmic Form</h4><ul>
<li>åªå½±å“æˆªè·ï¼Œä¸å½±å“ç³»æ•°</li>
</ul>
<h4 id="Beta-Coefficients"><a href="#Beta-Coefficients" class="headerlink" title="Beta Coefficients"></a>Beta Coefficients</h4><ul>
<li>Sometimes, itâ€™s useful to obtain regression results when all variables are standardized: subtracting off its mean and dividing by its standard deviation.</li>
<li>$$<br>\begin{aligned}<br>y_i &amp; &#x3D;\hat{\beta_0}+\hat{\beta_1} x_{i 1}+\cdots+\hat{\beta_k} x_{i k}+\hat{u_i} . \\<br>(y_i-\bar{y}) &#x2F; \hat{\sigma_y} &amp; &#x3D;(\hat{\sigma_1} &#x2F; \hat{\sigma_y}) \hat{\beta_1}[(x_{i 1}-\bar{x_1}) &#x2F; \hat{\sigma_1}]+\cdots \\<br>&amp; +(\hat{\sigma_k} &#x2F; \hat{\sigma_y}) \hat{\beta_k}[(x_{i k}-\bar{x_k}) &#x2F; \hat{\sigma_k}]+(\hat{u_i} &#x2F; \hat{\sigma_y}) \\<br>z_y &amp; &#x3D;\hat{b_1} z_1+\cdots+\hat{b_k} z_k+\text { error }<br>\end{aligned}<br>$$</li>
<li>where $z_y$ denotes the z-score of $y$. The new coefficients are$$\hat{b}_j&#x3D;(\hat{\sigma}_j &#x2F; \hat{\sigma}_y) \hat{\beta}_j$$</li>
<li>If $x_1$ increases by one standard deviation, then $\hat{y}$ changes by $\hat{b}_1$ standard deviation.</li>
<li>å½’ä¸€åŒ–äº†</li>
</ul>
<h3 id="More-on-Functional-Form"><a href="#More-on-Functional-Form" class="headerlink" title="More on Functional Form"></a>More on Functional Form</h3><ul>
<li>å¯¹äºlogå½¢å¼ $$z&#x3D;log(y)&#x3D;\beta_0+\beta_1 x+u$$<ul>
<li>å½“xå˜åŒ–$\Delta$ æ—¶ï¼Œ$$\%\Delta E(y|x)&#x3D;100[exp(\beta_1\Delta)-1]$$</li>
<li>å½“ $\Delta$ è¶‹è¿‘0æ—¶ï¼Œ$$\%\Delta E(y|x)\approx 100\beta_1\Delta$$</li>
</ul>
</li>
<li>å¯¹äºxå–å€¼å°äº0çš„ï¼Œå¯ä»¥ä½¿ç”¨ inverse hyperbolic sine: $$IRS(x)&#x3D;arcsinh(x)&#x3D;log(x+\sqrt{x^2+1})$$</li>
<li>å½“xå¯¹yçš„å½±å“æ˜¯éçº¿æ€§çš„ï¼Œå¯ä»¥è€ƒè™‘äºŒæ¬¡æ–¹ã€‚</li>
</ul>
<h3 id="More-on-Goodness-of-Fit"><a href="#More-on-Goodness-of-Fit" class="headerlink" title="More on Goodness of Fit"></a>More on Goodness of Fit</h3><ul>
<li>åœ¨Ch4ä¸­é€šè¿‡Fæ£€éªŒæ¥åˆ¤æ–­æ˜¯å¦å¯ä»¥restrictæ¨¡å‹ï¼Œé‚£ä¹ˆå¯¹äºnon-nested modelæ€ä¹ˆåŠå‘¢ï¼Ÿ</li>
<li>ä¾‹å¦‚ï¼š $$\begin{aligned}<br>&amp; y&#x3D;\beta_0+\beta_1 x_1+\beta_2 x_2+u \\<br>&amp; y&#x3D;\gamma_0+\gamma_1 x_4+e<br>\end{aligned}$$</li>
<li>è¿™æ—¶å€™éœ€è¦ç”¨ Adjusted R-square<ul>
<li>é€‰æ‹© $\bar{R}^2$ æœ€é«˜çš„</li>
</ul>
</li>
</ul>
<h3 id="Prediction-Analysis"><a href="#Prediction-Analysis" class="headerlink" title="Prediction Analysis"></a>Prediction Analysis</h3><h4 id="confidence-interval-for-E-y-x"><a href="#confidence-interval-for-E-y-x" class="headerlink" title="confidence interval for E(y|x)"></a>confidence interval for E(y|x)</h4><ul>
<li>Suppose we have estimated the equation$$\hat{y}&#x3D;\hat{\beta}_0+\hat{\beta}_1 x_1+\hat{\beta}_2 x_2+\ldots+\hat{\beta}_k x_k$$Let $c_1, c_2, \ldots, c_k$ denote particular values for each of the $k$ independent variables.</li>
<li>The parameter we would like to estimate is$(\theta)&#x3D;E(y \mid x_1&#x3D;c_1, \ldots, x_k&#x3D;c_k)&#x3D;\beta_0+\beta_1 c_1+\beta_2 c_2+\ldots+\beta_k c_k$</li>
<li>The estimator of $\theta$ is$$\hat{\theta}&#x3D;\hat{\beta}_0+\hat{\beta}_1 c_1+\hat{\beta}_2 c_2+\ldots+\hat{\beta}_k c_k$$</li>
<li>$$y&#x3D;\theta+\beta_1(x_1-c_1)+\beta_2(x_2-c_2)+\ldots+\beta_k(x_k-c_k)+u$$</li>
</ul>
<ul>
<li>So we can regress $y_i$ on $(x_{i 1}-c_1), \ldots,(x_{i k}-c_k)$. The standard error and confidence interval of the intercept of this new regression is what we need.</li>
</ul>
<h4 id="Prediction-Interval"><a href="#Prediction-Interval" class="headerlink" title="Prediction Interval"></a>Prediction Interval</h4><ul>
<li>$$y&#x3D;E(y \mid x_1, \ldots, x_k)+u$$</li>
<li>The previous method form a confidence interval for $E(y \mid x_1, \ldots, x_k)$.</li>
</ul>
<ul>
<li>Sometimes we are interested in forming the confidence interval for an unknown outcome on $y$.</li>
<li>We need to account for the variation in $u$.</li>
</ul>
<ul>
<li>Let $x_1^0, \ldots, x_k^0$ be the new vales of the independent variables, which we assume we observe. Let $u^0$ be the unobserved error.<br>$$<br>y^0&#x3D;\beta_0+\beta_1 x_1^0+\ldots+\beta_k x_k^0+u^0 .<br>$$</li>
<li>Our best prediction of $y^0$ is estimated from the OLS regression line<br>$$<br>\hat{y}^0&#x3D;\hat{\beta}_0+\hat{\beta}_1 x_1^0+\ldots+\hat{\beta}_k x_k^0<br>$$</li>
<li>The prediction error in using $\hat{y}^0$ to predict $y^0$ is</li>
<li>Note $E(\hat{y}^0)&#x3D;y^0$, because the $\hat{\beta}_j$ are unbiased. Because $u^0$ has zero mean, $E\left(\hat{e}^0\right)&#x3D;0$.</li>
<li>Note that $u^0$ is uncorrelated with each $\hat{\beta}_j$, because $u^0$ is uncorrelated with the errors in the sample used to obtain the $\hat{\beta}_j$.</li>
</ul>
<ul>
<li>Therefore, the variance of the prediction error (conditional on all in-sample values of the independent variables) is:<br>$$<br>\operatorname{Var}(\hat{e}^0)&#x3D;\operatorname{Var}(\hat{y}^0)+\operatorname{Var}(u^0)&#x3D;\operatorname{Var}(\hat{y}^0)+\sigma^2 .<br>$$</li>
</ul>
<ul>
<li>The standard error of $\hat{e}^0$ is:$$se(\hat{e}^0)&#x3D;{[se(\hat{y}^0)]^2+\hat{\sigma}^2}^{1 &#x2F; 2}$$</li>
<li>The prediction interval for $y^0$ is<br>$$<br>\sqrt{\hat{y}^0 \pm c \cdot s e\left(\hat{e}^0\right)}<br>$$</li>
</ul>
<h2 id="Ch7-Multiple-Regression-Analysis-with-Qualitative-Information"><a href="#Ch7-Multiple-Regression-Analysis-with-Qualitative-Information" class="headerlink" title="Ch7 Multiple Regression Analysis with Qualitative Information"></a>Ch7 Multiple Regression Analysis with Qualitative Information</h2><h3 id="A-Single-Dummy-Independent-Variable"><a href="#A-Single-Dummy-Independent-Variable" class="headerlink" title="A Single Dummy Independent Variable"></a>A Single Dummy Independent Variable</h3><ul>
<li><p>We often capture binary information by defining binary variable or a zero-one variable.$$\text { female }&#x3D; \begin{cases}0, &amp; \text { if the individual is man } \\ 1, &amp; \text { if the individual is woman }\end{cases}$$</p>
</li>
<li><p>zero-one leads to natural interpretations of the regression parameters</p>
</li>
<li><p>å‡è®¾ $$\text { wage }&#x3D;\beta_0+\delta_0 \text { female }+u$$</p>
</li>
<li><p>If we estimate the model using OLS$$\begin{aligned}<br>&amp; \hat{\beta_0}&#x3D;\overline{w a g e_{m e n}} \\<br>&amp; \hat{\delta_0}&#x3D;\overline{w a g e_{women}}-\overline{w a g e_{m e n}}\end{aligned}$$</p>
</li>
<li><p>If we regress $y$ on a dummy variable $x$, then the OLS estimate of the intercept represents the sample average of $y$ when $x&#x3D;0$, the OLS estimate of the slope coefficient represents the difference between the sample average of $y$ when $x&#x3D;1$ and $x&#x3D;0$.</p>
</li>
<li><p>å½“å¢åŠ å˜é‡æ—¶ï¼Œä¾‹å¦‚ $$w a g e&#x3D;\beta_0+\delta_0 \text { female }+\beta_1 e d u c+u$$</p>
</li>
<li><p>$Î´_0$ is the difference in hourly wage between women and men, given the same amount of education.</p>
</li>
</ul>
<h4 id="çº¿æ€§ç›¸å…³"><a href="#çº¿æ€§ç›¸å…³" class="headerlink" title="çº¿æ€§ç›¸å…³"></a>çº¿æ€§ç›¸å…³</h4><ul>
<li>$wage&#x3D;\beta_0 + \beta_1female+u$ æ˜¯å¯ä»¥çš„<ul>
<li>é€‰å®šäº† male ä½œä¸º baseline group</li>
</ul>
</li>
<li>$wage&#x3D;\beta_0 male + \beta_1female+u$ æ˜¯å¯ä»¥çš„</li>
<li>$wage&#x3D;\beta_0+\beta_1 male + \beta_2female+u$ æ˜¯ä¸è¡Œçš„</li>
</ul>
<h4 id="Without-intercept"><a href="#Without-intercept" class="headerlink" title="Without intercept"></a>Without intercept</h4><ul>
<li>$wage&#x3D;\beta_0 male + \beta_1female+u$ æ²¡æœ‰æˆªè·ï¼Œåœ¨è§£é‡Šå’Œè®¡ç®—æ£€éªŒçš„æ—¶å€™ä¸æ–¹ä¾¿ï¼Œå¹¶ä¸” $R^2$ å¯èƒ½æ˜¯è´Ÿçš„ã€‚</li>
<li>In general, if there is no intercept in the regression model, the $R^2$ could be negative.</li>
<li>To address the issue, some researchers use the <strong>uncentered R-squared</strong> when there is no intercept in the model$$R_0^2&#x3D;1-\frac{S S R}{S S T_0},$$where $S S T_0&#x3D;\sum_{i&#x3D;1}^N y_i^2$.</li>
</ul>
<h3 id="Using-Dummy-Variables-for-Multiple-Categories"><a href="#Using-Dummy-Variables-for-Multiple-Categories" class="headerlink" title="Using Dummy Variables for Multiple Categories"></a>Using Dummy Variables for Multiple Categories</h3><ul>
<li><p>å¤šä¸ªç»„åˆ«çš„æ—¶å€™ï¼Œä¸¤ç§é€‰æ‹©</p>
</li>
<li><p>ç¬¬ä¸€ç§ï¼Œk-1ä¸ªç‹¬ç«‹çš„å˜é‡ï¼š</p>
<ul>
<li>$$\text { wage }&#x3D;\beta_0+\beta_1 \text { marrmale }+\beta_2 \text { marrfemale }+\beta_3 \text { singfem }+u \text {. }$$</li>
</ul>
</li>
<li><p>ç¬¬äºŒç§ï¼Œä¹˜ç§¯é¡¹ï¼š</p>
<ul>
<li>$$\text { wage }&#x3D;\beta_0+\beta_1 \text { female }+\beta_2 \text { married }+\beta_3 \text { female } \cdot \text { married }+u$$</li>
</ul>
</li>
<li><p>æ³¨æ„å¦‚æœä¸€ä¸ªå˜é‡åªèƒ½åœ¨å‡ ä¸ªç¦»æ•£å€¼ä¹‹é—´é€‰æ‹©ï¼Œæœ€å¥½æŠŠæ¯ä¸ªå€¼çš„æƒ…å†µç‹¬ç«‹æˆä¸€ä¸ªå“‘å…ƒï¼Œå¦åˆ™ç›¸å½“äºæš—ç¤ºäº†æ–œç‡å˜åŒ–æ˜¯çº¿æ€§çš„ã€‚</p>
<ul>
<li>ä¾‹å¦‚CRå¯ä»¥å–0ï¼Œ1ï¼Œ2ï¼Œ3ï¼Œ4</li>
<li>$$MBR&#x3D;\beta_0+\beta_1 C R+\text { other factors }+u \text {. }$$</li>
<li>$$MBR&#x3D;\beta_0+\delta_1 C R 1+\delta_2 C R 2+\delta_3 C R 3+\delta_4 C R 4+\text{other factors}+u$$</li>
<li>ç¬¬äºŒç§æ¨¡å‹æ›´å¥½ã€‚</li>
</ul>
</li>
</ul>
<h3 id="Interactions-Involving-Dummy-Variables"><a href="#Interactions-Involving-Dummy-Variables" class="headerlink" title="Interactions Involving Dummy Variables"></a>Interactions Involving Dummy Variables</h3><ul>
<li>åœ¨ $wage&#x3D;\beta_0+\beta_1female+\beta_2educ+u$ ä¸­æˆ‘ä»¬å‡å®šäº† educ å¯¹äºç”·å¥³çš„å½±å“æ˜¯ç›¸åŒçš„ã€‚</li>
<li>ä¸ºäº†åŒºåˆ«ï¼Œéœ€è¦æ·»åŠ ä¸€ä¸ªä¹˜ç§¯é¡¹ï¼š$$E(\text { wage } \mid \text { female }, \text { educ })&#x3D;\beta_0+\delta_0 \text { female }+\beta_1 \text { educ }+\delta_1 \text { female } \cdot \text { educ. }$$</li>
</ul>
<h4 id="Testing-for-Differences-in-Regression-Functions-across-Groups"><a href="#Testing-for-Differences-in-Regression-Functions-across-Groups" class="headerlink" title="Testing for Differences in Regression Functions across Groups"></a>Testing for Differences in Regression Functions across Groups</h4><ul>
<li>$$\text { wage }&#x3D;\beta_0+\beta_1 \text { educ }+\beta_2 \text { exper }+\beta_3 \text { tenure }+u$$</li>
</ul>
<ul>
<li>We want to test whether all the coefficients are the same for men and women.</li>
<li>We can include interactive terms for all variables:$$<br>\begin{aligned}<br>\text { wage }&#x3D; &amp; \beta_0+\delta_0 \text { female }+\beta_1 \text { educ }+\delta_1 \text { educ } \cdot \text { female }+ \\<br>&amp; \beta_2 \text { exper }+\delta_2 \text { exper } \cdot \text { female }+ \\<br>&amp; \beta_3 \text { tenure }+\delta_3 \text { tenure } \cdot \text { female }+u .<br>\end{aligned}$$</li>
</ul>
<ul>
<li>The null hypothesis:$$H_0: \delta_0&#x3D;0, \delta_1&#x3D;0, \delta_2&#x3D;0, \delta_3&#x3D;0$$We can use F test to test the hypothesis: estiamte the unrestricted and restricted model, and then calculate the F-stat.</li>
</ul>
<h4 id="Chow-statistic"><a href="#Chow-statistic" class="headerlink" title="Chow statistic"></a>Chow statistic</h4><ul>
<li>å¯¹äºåªæœ‰ä¸€ä¸ªäºŒå…ƒå˜é‡å’Œå¾ˆå¤šå…¶ä»–è¿ç»­å˜é‡çš„å›å½’ï¼Œæˆ‘ä»¬æƒ³åˆ¤æ–­å…¶ä»–æ‰€æœ‰çš„å˜é‡æ˜¯å¦å…³äºä¸¤ç»„å®Œå…¨ç›¸åŒ</li>
<li>We can show that the sum of squared residuals from the unrestricted model can be obtained from two separate regressions, one for each group: $S S R_{u r}&#x3D;S S R_1+S S R_2$</li>
<li>The F-statstic:$$F&#x3D;\frac{S S R_p-(S S R_1+S S R_2)}{S S R_1+S S R_2} \cdot \frac{N-2(k+1)}{k+1}$$$S S R_p$ : SSR from pooling the groups and estimating a single equation. </li>
<li>This is also called a Chow statistic.</li>
<li>Note: use the Chow test if<ul>
<li>the model satisfies homoskedasticity</li>
<li>we want to test no differences at all between the groups</li>
</ul>
</li>
</ul>
<h3 id="Program-Evaluation"><a href="#Program-Evaluation" class="headerlink" title="Program Evaluation"></a>Program Evaluation</h3><ul>
<li>åœ¨ç¤¾ä¼šå­¦å®éªŒä¸­æ§åˆ¶å˜é‡æ³•</li>
</ul>
<h2 id="Ch8-Heteroskedasticity"><a href="#Ch8-Heteroskedasticity" class="headerlink" title="Ch8 Heteroskedasticity"></a>Ch8 Heteroskedasticity</h2><h3 id="Consequence-of-Heteroskedasticity-for-OLS"><a href="#Consequence-of-Heteroskedasticity-for-OLS" class="headerlink" title="Consequence of Heteroskedasticity for OLS"></a>Consequence of Heteroskedasticity for OLS</h3><ul>
<li>Heteroskedasticity does not cause bias or inconsistency in the OLS estimators<ul>
<li>å¯¹äºä¸€è‡´æ€§å’Œæ— åæ€§æ— å½±å“</li>
</ul>
</li>
<li>The interpretation of our goodness-of-fit measures is also unaffected by the presence of heteroskedasticity.<ul>
<li>å¯¹äºgoodness of fit æ— å½±å“</li>
</ul>
<ul>
<li>$R^2$ and adj- $R^2$ are different ways of estimating the population R-squared, $1-\sigma_u^2 &#x2F; \sigma_y^2$.</li>
<li>both variances in the population $R^2$ are unconditional variances</li>
<li>SSR&#x2F; $N$ consistently estimates $\sigma_u^2$, and $S S T &#x2F; N$ consistently estimates $\sigma_y^2$, whether or $\operatorname{not} \operatorname{Var}(u \mid x)$ is constant</li>
</ul>
</li>
</ul>
<ul>
<li>With heteroskedasticity, $\operatorname{Var}\left(\hat{\beta}_j\right)$ is biased.<ul>
<li>å¯¹äºæ–¹å·®æœ‰å½±å“</li>
<li>æ ‡å‡†å·®ï¼Œtæ•°æ®ï¼Œç½®ä¿¡åŒºé—´éƒ½ä¸å†å¯é </li>
<li>å¤§æ ·æœ¬ä¹Ÿä¸èƒ½è§£å†³</li>
<li>OLS is no longer BLUE.</li>
</ul>
</li>
</ul>
<h3 id="Heteroskedasticity-Robust-Inference-after-OLS-Estimation"><a href="#Heteroskedasticity-Robust-Inference-after-OLS-Estimation" class="headerlink" title="Heteroskedasticity-Robust Inference after OLS Estimation"></a>Heteroskedasticity-Robust Inference after OLS Estimation</h3><ul>
<li>Consider the simple linear regression model:$$y_i&#x3D;\beta_0+\beta_1 x_i+u_i$$Assume SLR.1-SLR.4 are satisfied, and there exists heteroskedasticity:$$\operatorname{Var}(u \mid x_i)&#x3D;\sigma_i^2$$$\operatorname{Var}(u)$ takes on different values when $x$ varies</li>
</ul>
<ul>
<li>We donâ€™t know the exact functional form of $\sigma_i^2$, it can be any function of $x$</li>
</ul>
<h4 id="Estimating-operatorname-Var-left-hat-beta-j-right-under-Heteroskedasticity"><a href="#Estimating-operatorname-Var-left-hat-beta-j-right-under-Heteroskedasticity" class="headerlink" title="Estimating $\operatorname{Var}\left(\hat{\beta}_j\right)$ under Heteroskedasticity"></a>Estimating $\operatorname{Var}\left(\hat{\beta}_j\right)$ under Heteroskedasticity</h4><ul>
<li>One valid estimator ï¼ˆWhite,1980ï¼‰:$$\widehat{\operatorname{Var}}(\hat{\beta_1})&#x3D;\frac{\sum_{i&#x3D;1}^N(x_i-\bar{x})^2 \hat{u_i}^2}{[\sum_{i&#x3D;1}^N(x_i-\bar{x})^2]^2} \equiv \frac{\sum_{i&#x3D;1}^N(x_i-\bar{x})^2 \hat{u_i}^2}{SST_x^2}$$where $$SST_x&#x3D;\sum_{i&#x3D;1}^N(x_i-\bar{x})^2$$.</li>
<li>For multiple regression model:$$\begin{gathered}y_i&#x3D;\beta_0+\beta_1 x_{i 1}+\beta_2 x_{i 2}+\ldots+\beta_k x_{i k}+u_i . \\ \operatorname{Var}(\hat{\beta_j})&#x3D;\frac{\sum_{i&#x3D;1}^N \hat{r_{ij}}^2 \sigma_i^2}{[\sum_{i&#x3D;1}^N \hat{r_{ij}}^2]^2} \equiv \frac{\sum_{i&#x3D;1}^N \hat{r_{ij}}^2 \sigma_i^2}{SSR_j^2}\end{gathered}$$The estimator:$$\widehat{\operatorname{Var}}(\hat{\beta_j})&#x3D;\frac{\sum_{i&#x3D;1}^N \hat{r_{ij}}^2 \hat{u_i}^2}{[\sum_{i&#x3D;1}^N \hat{r_{ij}}^2]^2}&#x3D;\frac{\sum_{i&#x3D;1}^N \hat{r_{ij}}^2 \hat{u_i}^2}{SSR_j^2}$$<ul>
<li>$\hat{r_{ij}}$ is the residual from regressing $x_j$ on all other independent variables</li>
<li>$S S R_j$ is the sum of residual squared of this regression</li>
<li>The square root of $\widehat{\operatorname{Var}}\left(\hat{\beta}_j\right)$ is called the heteroskedasticity-robust standard error, or simply, robust standard errors.</li>
</ul>
</li>
<li>Robust-standard error æ˜¯ä¸€è‡´çš„</li>
</ul>
<h4 id="Compare-the-variance-formula"><a href="#Compare-the-variance-formula" class="headerlink" title="Compare the variance formula"></a>Compare the variance formula</h4><ul>
<li>Under homoskedasticity, $\operatorname{Var}(\hat{\beta_1})$ is simplified as$$\operatorname{Var}(\hat{\beta_j})&#x3D;\frac{\sum_{i&#x3D;1}^N \hat{r_{ij}}^2 \sigma_i^2}{SSR_j^2}&#x3D;\frac{\sigma^2}{SSR_j}$$</li>
<li>Under heteroskedasticity,$$\operatorname{Var}(\hat{\beta_j})&#x3D;\frac{\sum_{i&#x3D;1}^N r_{i j}^2 \sigma_i^2}{SSR_j^2}&#x3D;\frac{1}{SSR_j} \sum_{i&#x3D;1}^N \frac{\hat{r_{ij}}^2}{SSR_j} \sigma_i^2&#x3D;\frac{1}{SSR_j} \sum_{i&#x3D;1}^N w_{ij} \sigma_i^2$$where $w_{ij}&#x3D;\frac{\hat{\tau_{ij}}^2}{S S R_j}$. We know that $w_{ij}&gt;0$ and $\sum_{i&#x3D;1}^N w_{ij}&#x3D;1$.</li>
<li>å³ï¼Œè¿›è¡Œäº†åŠ æƒå¹³å‡ã€‚</li>
<li>Robust standard errors can be either larger or smaller than the usual standard errors.</li>
</ul>
<h4 id="More-on-RSE"><a href="#More-on-RSE" class="headerlink" title="More on RSE"></a>More on RSE</h4><ul>
<li>åœ¨æœ‰äº›æƒ…å†µä¸‹ï¼Œç‰¹åˆ«æ˜¯å¼‚æ–¹å·®æ€§ä¸å¼ºçš„æƒ…å†µä¸‹ï¼Œrobust-standard-errorçš„è¡¨ç°ä¸å¦‚ä¼ ç»Ÿçš„standard-error<ul>
<li>å°æ ·æœ¬æ—¶rseå­˜åœ¨è¯¯å·®</li>
<li>rseçš„æ ·æœ¬æ–¹å·®æ›´å¤§</li>
</ul>
</li>
<li>åœ¨å®è·µä¸­é€šå¸¸åœ¨å¤§æ ·æœ¬æ—¶æŠ¥å‘Šrseï¼Œåœ¨å°æ ·æœ¬æ—¶éƒ½æŠ¥å‘Šã€‚</li>
</ul>
<h3 id="Weighted-Least-Squares-Estimation"><a href="#Weighted-Least-Squares-Estimation" class="headerlink" title="Weighted Least Squares Estimation"></a>Weighted Least Squares Estimation</h3><h4 id="Generalized-Least-Squares-ï¼ˆGLSï¼‰"><a href="#Generalized-Least-Squares-ï¼ˆGLSï¼‰" class="headerlink" title="Generalized Least Squares ï¼ˆGLSï¼‰"></a>Generalized Least Squares ï¼ˆGLSï¼‰</h4><ul>
<li>Assume MLR.1-MLR.4 are satisfied:$$y_i&#x3D;\beta_0+\beta_1 x_{i 1}+\ldots+\beta_k x_{i k}+u_i$$</li>
<li>Assume that the variance of $u$ takes the following form:$$\operatorname{Var}(u \mid x_1, \ldots, x_k)&#x3D;\sigma^2 h(x_1, \ldots, x_k)$$</li>
<li>We write $\sigma_i^2&#x3D;\sigma^2 h\left(x_{i 1}, \ldots, x_{i k}\right)&#x3D;\sigma^2 h_i$.</li>
</ul>
<ul>
<li>Consider an alternative regression model:$$\frac{y_i}{\sqrt{h_i}}&#x3D;\beta_0 \frac{1}{\sqrt{h_i}}+\beta_1 \frac{x_{i 1}}{\sqrt{h_i}}+\ldots+\beta_k \frac{x_{i k}}{\sqrt{h_i}}+\frac{u_i}{\sqrt{h_i}}$$</li>
</ul>
<ul>
<li>Let $\mathbf{x}$ denote all the explanatory variables. Conditional on $\mathbf{x}, E\left(u_i &#x2F; \sqrt{h_i} \mid \mathbf{x}\right)&#x3D;E\left(u_i \mid \mathbf{x}\right) &#x2F; \sqrt{h_i}&#x3D;0$.</li>
<li>$\operatorname{Var}\left(u_i &#x2F; \sqrt{h_i} \mid \mathbf{x}\right)&#x3D;\sigma^2$, satisfying homoskedasticity.</li>
<li>Denote the OLS estimator after the transformation as ${\beta_j^*}$</li>
<li>We can prove that ${\beta_j^*}$ minimizes$$\sum_{i&#x3D;1}^N(y_i-b_0-b_1 x_{i 1}-\cdots-b_k x_{i k})^2 &#x2F; h_i$$</li>
<li>Weighted least squares estimatorï¼ˆWLSï¼‰: <ul>
<li>the weight for each $\hat{u}_i$ is $1 &#x2F; h_i$. We give less weight for observations with higher variance. Intuitively, they provide less information.</li>
</ul>
</li>
<li>${\beta_j^*}$ is still one estimator for the original model, and have the same interpretation</li>
<li>Because ${\beta_j^*}$ satisfies MLR.1-MLR.5, so it is BLUE under heteroskedasticity with the form $\sigma_i^2&#x3D;\sigma^2 h_i$</li>
<li>${\beta_j^*}$ is also called generalized least squares estimators ï¼ˆGLSï¼‰</li>
</ul>
<h4 id="Feasible-Generalized-Least-Squares-ï¼ˆFGLSï¼‰"><a href="#Feasible-Generalized-Least-Squares-ï¼ˆFGLSï¼‰" class="headerlink" title="Feasible Generalized Least Squares ï¼ˆFGLSï¼‰"></a>Feasible Generalized Least Squares ï¼ˆFGLSï¼‰</h4><ul>
<li>å®é™…ä¸­éœ€è¦ä¼°è®¡ $h_i$</li>
</ul>
<ul>
<li>Assume $h_i$ takes the following form:$$\begin{aligned}\operatorname{Var}(u \mid x) &amp; &#x3D;\sigma^2 \exp \left(\delta_0+\delta_1 x_1+\ldots+\delta x_k\right) \\ u^2 &amp; &#x3D;\sigma^2 \exp \left(\delta_0+\delta_1 x_1+\ldots+\delta x_k\right) v,\end{aligned}$$where $v$ has a mean of one.</li>
<li>We take $\exp (\cdot)$ to guarantee that $\operatorname{Var}(u)&gt;0$</li>
<li>Equivalently,<br>$$<br>\log \left(u^2\right)&#x3D;\alpha+\delta_1 x_1+\ldots+\delta x_k+e .<br>$$</li>
</ul>
<ul>
<li>As usual, we replace the unobserved $u$ with the OLS residuals $\hat{u}$, and estimate $\log \left(\hat{u}^2\right) \sim 1, x_1, \ldots x_k$, calculate the fitted value $\hat{g}_i$. Then $\hat{h}_i&#x3D;\exp \left(\hat{g}_i\right)$.</li>
</ul>
<h5 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h5><ol>
<li>Run the regression of $y$ on $1, x_1, \ldots, x_k$, get the residual $\hat{u}_i$</li>
<li>Calculate $\log \left(\hat{u}_i^2\right)$</li>
<li>Estimate $\log \left(\hat{u}_i^2\right) \sim 1, x_1, \ldots x_k$, get the fitted value $\hat{g}_i$</li>
<li>Compute $\hat{h}_i&#x3D;\exp \left(\hat{g}_i\right)$</li>
<li>Use $1 &#x2F; \hat{h}_i$ as weights, estimate $y \sim 1, x_1, \ldots, x_k$ using WLS.</li>
</ol>
<p>FGLS is consistent, and has smaller asymptotic variance than OLS.</p>
<h4 id="WLS-or-RSE"><a href="#WLS-or-RSE" class="headerlink" title="WLS or RSE"></a>WLS or RSE</h4><ul>
<li>There is no guarantee that WLS is more efficient than OLS.</li>
</ul>
<ul>
<li>It is alwasy advised to report robust standard errors with WLS.</li>
</ul>
<ul>
<li>two solutions for heteroskedasticity:<ul>
<li>Use OLS to estiamte the model, calculate the robust standard errors ï¼ˆor use the max of the conventional s.e. and robust s.e.ï¼‰</li>
<li>Use FGLS to estimate the model, report conventional s.e. or robust s.e.</li>
</ul>
</li>
<li>In practice, the first method is preferred in most cases</li>
</ul>
<h3 id="Testing-for-Heteroskedasticity"><a href="#Testing-for-Heteroskedasticity" class="headerlink" title="Testing for Heteroskedasticity"></a>Testing for Heteroskedasticity</h3><h4 id="Breusch-Pagan-Test-for-Heteroskedasticity"><a href="#Breusch-Pagan-Test-for-Heteroskedasticity" class="headerlink" title="Breusch-Pagan Test for Heteroskedasticity"></a>Breusch-Pagan Test for Heteroskedasticity</h4><ul>
<li>We want to know in model $y&#x3D;\beta_0+\beta_1 x_1+. .+\beta_k x_k+u$, whether $u^2$ is correlated with $x$</li>
<li>Estimate $y&#x3D;\beta_0+\beta_1 x_1+. .+\beta_k x_k+u$, get the residual $\hat{u}$</li>
<li>Estimate the following model and get $R_{\hat{u}^2}^2$ :$$\hat{u}_i^2&#x3D;\delta_0+\delta_1 x_1+\ldots+\delta_k x_k+v$$</li>
<li>We test $H_0: \delta_1&#x3D;\ldots&#x3D;\delta_k&#x3D;0$</li>
<li>Calculate the $L M$ statistic: $N \cdot R_{\hat{u}^2}^2$; or calculate the $F$ $\operatorname{statistic}\left[R_{\hat{u}^2}^2 &#x2F; k\right] &#x2F;\left[\left(1-R_{\hat{u}^2}^2\right) &#x2F;(N-k-1)\right]$.</li>
<li>Reject homoskedasticity if<ul>
<li>test statistic $&gt;$ critical value</li>
<li>$p&lt;$ significance level</li>
</ul>
</li>
</ul>
<h4 id="The-White-Test-for-Heteroskedasticity"><a href="#The-White-Test-for-Heteroskedasticity" class="headerlink" title="The White Test for Heteroskedasticity"></a>The White Test for Heteroskedasticity</h4><ul>
<li>OLS standard errors are asymptotically valid if MLR.1-MLR.5 holds.</li>
<li>It turns out that the homoskedasticity assumption can be replaced with the weaker assumption that the squared error, $u^2$, is uncorrelated with all the independent variables $\left(x_j\right)$, the squares of the independent variables $\left(x_j^2\right)$, and all the cross products $\left(x_j x_h, \forall j \neq h\right)$.</li>
</ul>
<ul>
<li>When the model contain $k&#x3D;2$ independent variables, the White test is based on an estimation of$$\hat{u}^2&#x3D;\delta_0+\delta_1 x_1+\delta_2 x_2+\delta_3 x_1^2+\delta_4 x_2^2+\delta_5 x_1 x_2+v$$The White test for heteroskedasticity is the LM statistic for testing that all of the $\delta_j$ are zero, except for the intercept.</li>
<li>Problem: with many independent variables, we uses many degrees of freedom. Solution: use $\hat{y}^2$ :$$\hat{u}^2&#x3D;\delta_0+\delta_1 \hat{y}+\delta_2 \hat{y}^2+v$$We then use the $\mathrm{F}$ or LM statistic for the null hypothesis $H_0: \delta_0&#x3D;\delta_2&#x3D;0$.</li>
</ul>
<h2 id="Ch12-Serial-Correlation"><a href="#Ch12-Serial-Correlation" class="headerlink" title="Ch12 Serial Correlation"></a>Ch12 Serial Correlation</h2><h3 id="Serial-Correlation"><a href="#Serial-Correlation" class="headerlink" title="Serial Correlation"></a>Serial Correlation</h3><h4 id="Times-series-data"><a href="#Times-series-data" class="headerlink" title="Times series data"></a>Times series data</h4><ul>
<li>Time series data: observations on variables over time. </li>
<li>random sampling is often violated</li>
</ul>
<h4 id="Classical-Assumptions-about-Time-Series-Data"><a href="#Classical-Assumptions-about-Time-Series-Data" class="headerlink" title="Classical Assumptions about Time Series Data"></a>Classical Assumptions about Time Series Data</h4><ol>
<li>The stochastic process ${(x_{t 1}, \ldots, x_{t k}, y_t): t&#x3D;1,2, \ldots, T}$ follows the linear model:$$y_t&#x3D;\beta_0+\beta_1 x_{t 1}+\ldots+\beta_k x_{t k}+u_t$$</li>
<li>No perfect collinearity.</li>
<li>Zero conditional mean.$$E(u_t \mid \mathbf{X})&#x3D;0, t&#x3D;1,2, \ldots, T$$<ul>
<li>where $\mathbf{X}$ is the explanatory variables for all time periods.</li>
<li>$E\left(u_t \mid \mathbf{X}\right)&#x3D;0$ means both $E\left(u_t \mid x_t\right)&#x3D;0$ and also $E\left(u_t \mid x_s\right)&#x3D;0, \forall t \neq s$.</li>
</ul>
</li>
</ol>
<ul>
<li>Unbiasedness of $\mathrm{OLS}$<ul>
<li>Under assumptions TS.1, TS.2 and TS.3, the OLS estimators are unbiased and consistent.</li>
</ul>
</li>
</ul>
<h4 id="Serial-Correlation-1"><a href="#Serial-Correlation-1" class="headerlink" title="Serial Correlation"></a>Serial Correlation</h4><ul>
<li>No serial correlation assumption:$$\operatorname{Cov}((x_t-\bar{x}) u_t,(x_s-\bar{x}) u_s \mid X)&#x3D;0, \forall t \neq s$$Or$$E(u_s u_t \mid X)&#x3D;0, \forall t \neq s$$</li>
<li>For time-series data, this is often not true.</li>
</ul>
<h4 id="Auto-regressionï¼ŒAR"><a href="#Auto-regressionï¼ŒAR" class="headerlink" title="Auto-regressionï¼ŒAR"></a>Auto-regressionï¼ŒAR</h4><ul>
<li>Think about a simple regression model:$$y_t&#x3D;\beta_0+\beta_1 x_t+u_t$$</li>
<li>Assume that$$u_t&#x3D;\rho u_{t-1}+e_t, t&#x3D;1,2, \ldots, T$$where $|\rho|&lt;1$, and $e_t$ are i.i.d with $E\left(e_t\right)&#x3D;0$. This is called an autoregressive process of order one $(\operatorname{AR}(1))$.</li>
</ul>
<h5 id="Properties-of-AR"><a href="#Properties-of-AR" class="headerlink" title="Properties of AR"></a>Properties of AR</h5><ul>
<li>Because $e_t$ is i.i.d, $u_t$ will be correlated with current and past $e_t$, but not future values. If the time series has been going on forever$$u_t &#x3D;\rho u_{t-1}+e_t&#x3D;\rho^k u_{t-k}+\rho^{k-1} e_{t-(k-1)}+\ldots+e_t &#x3D;\sum_{j&#x3D;0}^{\infty} \rho^j e_{t-j}$$</li>
<li>$$E(u_t) &#x3D;E(\sum_{j&#x3D;0}^{\infty} \rho^j e_{t-j})&#x3D;\sum_{j&#x3D;0}^{\infty} \rho^j E(e_{t-j})&#x3D;0$$</li>
<li>We can show that$$\begin{aligned}<br>\operatorname{Var}(u_t) &amp; &#x3D;\operatorname{Var}(\sum_{j&#x3D;0}^{\infty} \rho^j e_{t-j})&#x3D;\sum_{j&#x3D;0}^{\infty} \rho^{2 j} \operatorname{Var}(e_{t-j}) \\<br>&amp; &#x3D;\operatorname{Var}(e_t) \sum_{j&#x3D;0}^{\infty} \rho^{2 j}&#x3D;\frac{\operatorname{Var}(e_t)}{1-\rho^2}\end{aligned}$$</li>
<li>Also$$\begin{aligned}\operatorname{Cov}(u_t, u_{t+1}) &amp; &#x3D;\operatorname{Cov}(u_t, \rho u_t+e_t)&#x3D;\rho \operatorname{Var}(u_t) \\<br>\operatorname{Cov}(u_t, u_{t+j}) &amp; &#x3D;\rho^j \operatorname{Var}(u_t)<br>\end{aligned}$$</li>
<li>Assume further that $\bar{x}&#x3D;0$ and homoskedasticity, that is $\operatorname{Var}\left(u_t \mid X\right)&#x3D;\operatorname{Var}\left(u_t\right)&#x3D;\sigma^2$. Then</li>
<li>$$<br>\begin{aligned}<br>\operatorname{Var}(\hat{\beta} \mid \mathbf{X}) &amp; &#x3D;\frac{\operatorname{Var}(\sum_{t&#x3D;1}^T x_t u_t \mid \mathbf{X})}{S S T_x^2} \\<br>&amp; &#x3D;\frac{\sum_{t&#x3D;1}^T x_t^2 \operatorname{Var}(u_t)+2 \sum_{t&#x3D;1}^{T-1} \sum_{j&#x3D;1}^{T-t} x_t x_{t+j} E(u_t u_{t+j})}{S S T_x^2} \\<br>&amp; &#x3D;\frac{\sigma^2}{S S T_x}+\frac{2 \sigma^2}{S S T_x^2} \sum_{t&#x3D;1}^{T-1} \sum_{j&#x3D;1}^{T-t} \rho^j x_t x_{t+j}<br>\end{aligned}$$</li>
</ul>
<h4 id="Consequence-of-ignore-serial-correlation"><a href="#Consequence-of-ignore-serial-correlation" class="headerlink" title="Consequence of ignore serial correlation"></a>Consequence of ignore serial correlation</h4><ul>
<li>ä»ç„¶æ˜¯æ— åã€ä¸€è‡´çš„</li>
<li>ä½†ä¼ ç»Ÿçš„æ–¹å·®æœ‰é—®é¢˜äº†</li>
<li>ä¼šä½ä¼°æ–¹å·®</li>
<li>å®Œå–„æ–¹æ³•ï¼š<ol>
<li>ä½¿ç”¨FGLS</li>
<li>ä½¿ç”¨OLSï¼Œä¿®æ­£se</li>
</ol>
</li>
</ul>
<h5 id="FGLS"><a href="#FGLS" class="headerlink" title="FGLS"></a>FGLS</h5><ul>
<li>Assume TS.1-TS.3. Further, assume $\operatorname{Var}\left(u_t \mid X\right)&#x3D;\sigma^2$.$$<br>\begin{aligned}<br>&amp; y_t&#x3D;\beta_0+\beta_1 x_t+u_t . \\<br>&amp; u_t&#x3D;\rho u_{t-1}+e_t, t&#x3D;1,2, \ldots, T .<br>\end{aligned}$$<ul>
<li>where $e_t$ is i.i.d and $E\left(e_t\right)&#x3D;0$.</li>
</ul>
</li>
<li>Transform the regression:$$\begin{aligned}<br>y_t-\rho y_{t-1} &amp; &#x3D;(1-\rho) \beta_0+\beta_1(x_t-\rho x_{t-1})+e_t, t \geq 2 . \\<br>\tilde{y}_t &amp; &#x3D;(1-\rho) \beta_0+\beta_1 \tilde{x}_t+e_t, t \geq 2\end{aligned}$$<br>We can use FGLS to estimate:</li>
</ul>
<ol>
<li>Estimate the model using OLS and obtain the OLS residuals $\hat{u}_t$</li>
<li>Use OLS to estimate $\hat{u_t} \sim \hat{u_{t-1}}$ and obtain $\hat{\rho}$.</li>
<li>Calculate $\tilde{y_t}&#x3D;y_t-\hat{\rho} y_{t-1}$ and $\tilde{x_t}&#x3D;x_t-\hat{\rho} x_{t-1}$, then use OLS to regress $\tilde{y_t}$ on $\tilde{x_t}$.</li>
</ol>
<h5 id="Serial-Correlation-Robust-Inference-after-OLS"><a href="#Serial-Correlation-Robust-Inference-after-OLS" class="headerlink" title="Serial Correlation-Robust Inference after OLS"></a>Serial Correlation-Robust Inference after OLS</h5><ul>
<li>äº†è§£å³å¯ï¼Œè®°ä½HACï¼ˆheteroskedasticity and auto-correlation consistentï¼‰</li>
<li>We can show that$$AVar(\hat{\beta_1})&#x3D;(\sum_{t&#x3D;1}^T E(r_t^2))^{-2} Var(\sum_{t&#x3D;1}^T r_t u_t)$$where $r_t$ is the error term in $x_{t 1}&#x3D;\delta_0+\delta_2 x_{t 2}+\ldots+\delta_k x_{t k}+r_t$. We want to find and estimator for $A \operatorname{Var}(\hat{\beta}_1)$.</li>
<li>Let $\hat{r}_t$ denote the residuals from regressing $x_1$ on all other independent variables, and $\hat{u}_t$ as the OLS residual from regressing $y$ on all $x$.</li>
<li>Define$$\hat{\nu}&#x3D;\sum_{t&#x3D;1}^T \hat{a_t}^2+2 \sum_{h&#x3D;1}^g[1-h &#x2F;(g+1)](\sum_{t&#x3D;h+1}^T \hat{a_t} \hat{a_{t-h}}),$$where $\hat{a_t}&#x3D;\hat{r_t} \hat{u_t}$.</li>
<li>Then$$s e(\hat{\beta_1})&#x3D;[se_c(\hat{\beta_1}) &#x2F; \hat{\sigma}]^2 \sqrt{\hat{\nu}}$$where $se_c(\hat{\beta_1})$ is the conventional standard error of $\hat{\beta}_1$, and $\hat{\sigma}$ is the square root of the sum of the OLS residual squared.</li>
<li>We use $g$ to capture how much serial correlation we are allowing in computing the standard error.</li>
<li>For annual data, choose $g&#x3D;1$ or $g&#x3D;2$</li>
<li>Use a larger $g$ for larger sample size.</li>
<li>When $g&#x3D;1$,$$\hat{\nu}&#x3D;\sum_{t&#x3D;1}^T \hat{a_t}^2+\sum_{t&#x3D;2}^T(\hat{a_t} \hat{a_{t-1}})$$</li>
<li>This formula is robust to arbitrary serial correlation and arbitrary heteroskedasticity. So people sometimes call this heteroskedasticity and auto-correlation consistent, or HAC, standard errors.</li>
</ul>
<h3 id="Spatial-Correlation"><a href="#Spatial-Correlation" class="headerlink" title="Spatial Correlation"></a>Spatial Correlation</h3><h4 id="Data-with-group-structure"><a href="#Data-with-group-structure" class="headerlink" title="Data with group structure"></a>Data with group structure</h4><ul>
<li>group structure, ä¾‹å¦‚ä¸åŒç­çº§çš„å­¦ç”Ÿï¼Œåœ¨åŒç­ä¹‹å†…æ˜¯æœ‰ç›¸å…³æ€§çš„</li>
<li>Example: class size and test score$$y_{i g}&#x3D;\beta_0+\beta_1 x_g+u_{i g}$$</li>
<li>Use $i$ to denote student, who are randomly assign to different class $g . y_{i g}$ is the test score of student $i$ ï¼ˆwho is in class $g$ ï¼‰, $x_g$ is the class size ï¼ˆwhich has the same value for students in the same class.ï¼‰</li>
<li>Assume that $E(u \mid X)&#x3D;0$</li>
</ul>
<ul>
<li>However, observations within the same $g$ is not independent ï¼ˆstudents in the same class are exposed to the same teacher and classroomâ€¦ï¼‰$$E(u_{i g} u_{j g})&#x3D;\rho_u \sigma_u^2 \neq 0$$</li>
</ul>
<ul>
<li>We call $\rho_u$ intraclass correlation coefficient.ã€</li>
<li>è¿™ç§ç›¸å…³æ€§å°±å« spatial correlation</li>
<li>å­˜åœ¨è¿™ç§æƒ…å†µæ—¶ï¼Œä¸€è‡´æ€§å’Œæ— åæ€§è¿˜æ˜¯ä¿è¯çš„ï¼Œä½†æ–¹å·®å’Œæ ‡å‡†å·®æœ‰å˜åŒ–ã€‚</li>
</ul>
<h4 id="Fix-spatial-correlation"><a href="#Fix-spatial-correlation" class="headerlink" title="Fix spatial correlation"></a>Fix spatial correlation</h4><h5 id="OLS-and-Cluster-Standard-Errors"><a href="#OLS-and-Cluster-Standard-Errors" class="headerlink" title="OLS and Cluster Standard Errors"></a>OLS and Cluster Standard Errors</h5><ul>
<li>The general idea is to model correlation of error terms within a group, and assume no correlation across groups. </li>
<li>groupæ•°é‡å˜å¤šçš„æ—¶å€™æ˜¯consistentçš„</li>
<li>å½“æ•°é‡å¤§äº42çš„æ—¶å€™å°±å¯ä»¥è®¤ä¸ºgroupæ•°é‡å¤Ÿå¤šäº†</li>
</ul>
<h5 id="Use-group-mean"><a href="#Use-group-mean" class="headerlink" title="Use group mean"></a>Use group mean</h5><ul>
<li>Estimate$$\bar{y}_g&#x3D;\beta_0+\beta_1 x_g+\bar{u}_g$$by WLS using the group size as weights.</li>
<li>We can generalize the method to models with microcovariates$$y_{i g}&#x3D;\beta_0+\beta_1 x_g+\beta_2 w_{i g}+u_{i g}$$<ol>
<li>Estimate$$y_{i g}&#x3D;\mu_g+\beta_2 w_{i g}+\eta_{i g}$$The group effects, $\mu_g$, are coefficients on a full set of group dummies.</li>
<li>Regress the estimated group effects on group-level variables$$\hat{\mu}_g&#x3D;\beta_0+\beta_1 x_g+e_g$$In this step, we could either weight by the group size, or use no weights.</li>
</ol>
</li>
</ul>
<h2 id="Ch9-Proxy-Variable-and-Measurement-Error"><a href="#Ch9-Proxy-Variable-and-Measurement-Error" class="headerlink" title="Ch9 Proxy Variable and Measurement Error"></a>Ch9 Proxy Variable and Measurement Error</h2><h3 id="Endogeneity-and-Exogeneity"><a href="#Endogeneity-and-Exogeneity" class="headerlink" title="Endogeneity and Exogeneity"></a>Endogeneity and Exogeneity</h3><ul>
<li>Zero conditional mean condition:$$E(u \mid x)&#x3D;0$$<ul>
<li>$x_j$ is endogenous if it is correlated with $u$.</li>
<li>$x_j$ is exogenous if it is not correlated with $u$.</li>
<li>Violating the zero conditional mean condition will cause the OLS estimator to be biased and inconsistent.</li>
</ul>
</li>
</ul>
<h3 id="Proxy-Variable"><a href="#Proxy-Variable" class="headerlink" title="Proxy Variable"></a>Proxy Variable</h3><ul>
<li>ä»£ç†å˜é‡</li>
</ul>
<h4 id="Omitted-Variable-Bias"><a href="#Omitted-Variable-Bias" class="headerlink" title="Omitted Variable Bias"></a>Omitted Variable Bias</h4><ul>
<li>$$\log (\text { wage })&#x3D;\beta_0+\beta_1 e d u c+\beta_2 a b i l+u$$</li>
<li>In this model, assume that $E(u \mid e d u c,abil)&#x3D;0$</li>
<li>å‡è®¾é¦–è¦ç›®çš„æ˜¯ä¼°è®¡ $\beta_1$ consistently,ä¸å…³æ³¨ $\beta_2$.</li>
</ul>
<ul>
<li>ä½†æˆ‘ä»¬æ²¡æœ‰å…³äºabilçš„æ•°æ®, æ‰€ä»¥åªç”¨ educ å›å½’ $\log ($ wage $)$ </li>
<li>There is an omitted variable bias if $\operatorname{cov}(abil,educ) \neq 0$ and $\beta_2 \neq 0$.</li>
</ul>
<ul>
<li>One solution: use proxy variable for the omitted variable</li>
</ul>
<ul>
<li>Proxy variable: related to the unobserved variable that we would like to control for in our analysis<ul>
<li>åªéœ€è¦è¿™ä¸ªå˜é‡proxy variableä¸abilç›¸å…³ï¼Œå³correlatedï¼Œä¸éœ€è¦å®Œå…¨ç›¸åŒ</li>
</ul>
</li>
</ul>
<h4 id="Proxy"><a href="#Proxy" class="headerlink" title="Proxy"></a>Proxy</h4><ul>
<li><p>Formally, we have a model$$y&#x3D;\beta_0+\beta_1 x_1+\beta_2 x_2^*+u$$</p>
</li>
<li><p>Assume that $E\left(u \mid x_1, x_2^*\right)&#x3D;0$</p>
</li>
<li><p>$x_1$ is observed and $x_2^*$ is unobserved</p>
</li>
<li><p>We have a proxy variable for $x_2^*$, which is $x_2$$$x_2^*&#x3D;\delta_0+\delta_2 x_2+v_2$$</p>
</li>
<li><p>where $v_2$ is the error to allow the possibility that $x_2$ and $x_2^*$ is not exactly related. $E\left(v_2 \mid x_2\right)&#x3D;0$.</p>
</li>
<li><p>Replace the omitted variable by the proxy variable:$$\color{red}{y&#x3D;(\beta_0+\beta_2 \delta_0)+\beta_1 x_1+\beta_2 \delta_2 x_2+(u+\beta_2 v_2)}$$To get an unbiased and consistent estimator for $\beta_1$, we require$$E(u+\beta_2 v_2 \mid x_1, x_2)&#x3D;0$$</p>
</li>
<li><p>Break this down into two assumptions:</p>
<ol>
<li>$E\left(u \mid x_1, x_2\right)&#x3D;0$ : the proxy variable should be exogenous ï¼ˆintuitively, since $x_2^*$ is exogenous, the proxy variable is only good if it is also exogenousï¼‰<ul>
<li>ä»£ç†å˜é‡éœ€è¦æ—¶å¤–ç”Ÿçš„</li>
</ul>
</li>
<li>$E\left(v_2 \mid x_1, x_2\right)&#x3D;0$ : this is equivalent as$$E(x_2^* \mid x_1, x_2)&#x3D;E(x_2^* \mid x_2)&#x3D;\delta_0+\delta_2 x_2$$Once $x_2$ is controlled for, the expected value of $x_2^*$ does not depend on $x_1$</li>
</ol>
</li>
<li><p>åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œå˜æˆï¼š$$\log (w a g e)&#x3D;\alpha_0+\alpha_1 e d u c+\alpha_2 I Q+e$$</p>
</li>
<li><p>In the wage equation example, the two assumptions are:</p>
<ol>
<li>$E(u \mid e d u c, I Q)&#x3D;0$</li>
<li>$E($ abil|educ, $I Q)&#x3D;E(a b i l \mid I Q)&#x3D;\delta_0+\delta_3 I Q$<ul>
<li>The average level of ability only changes with IQ, not with education ï¼ˆonce IQ is fixedï¼‰.</li>
</ul>
</li>
</ol>
</li>
<li><p><strong>åœ¨è¿™æ ·çš„å˜åŒ–ä¸­ï¼Œ$\beta_1$ æ˜¯æ— åçš„</strong></p>
<ul>
<li>è¿åå‡è®¾ï¼Œä¼šé€ æˆè¯¯å·®</li>
</ul>
</li>
</ul>
<h4 id="Using-Lagged-Dependent-Variables-as-Proxy-Variables"><a href="#Using-Lagged-Dependent-Variables-as-Proxy-Variables" class="headerlink" title="Using Lagged Dependent Variables as Proxy Variables"></a>Using Lagged Dependent Variables as Proxy Variables</h4><ul>
<li>æ»åå› å˜é‡</li>
<li>$$\text { crime }&#x3D;\beta_0+\beta_1 \text { unem }+\beta_2 \text { expend }+\beta_3 \text { crime }_{-1}+u$$</li>
<li>By including crime $_{-1}$ in the equation, $\beta_2$ captures the effect of expenditure of law enforcement on crime, for cities with the same previous crime rate and current unemployment rate.</li>
</ul>
<h3 id="Measurement-Error"><a href="#Measurement-Error" class="headerlink" title="Measurement Error"></a>Measurement Error</h3><h4 id="Measurement-Error-in-the-Dependent-Variable"><a href="#Measurement-Error-in-the-Dependent-Variable" class="headerlink" title="Measurement Error in the Dependent Variable"></a>Measurement Error in the Dependent Variable</h4><ul>
<li>å› å˜é‡çš„æµ‹é‡è¯¯å·®</li>
<li>Let $y^*$ denote the variable that we would like to explain.$$y^*&#x3D;\beta_0+\beta_1 x_1+\ldots+\beta_k x_k+u,$$and we assume it satisfies the Gauss-Markov assumptions.</li>
<li>Let $y$ to denote the observed measure of $y^*$</li>
<li>Measurement error is defined as$$e_0&#x3D;y-y^*$$</li>
<li>Plug in and rearrange$$y&#x3D;\beta_0+\beta_1 x_1+\ldots+\beta_k x_k+u+e_0$$</li>
<li>å½“eå’Œè‡ªå˜é‡éƒ½æ— å…³çš„æ—¶å€™ï¼Œç»“æœä»ç„¶æ˜¯ä¸€è‡´ä¸”æ— åçš„ï¼Œä½†æ–¹å·®ä¼šå˜å¤§</li>
<li>ä»ç„¶é€‚ç”¨OLS</li>
</ul>
<h4 id="Measurement-Error-in-the-Independent-Variable"><a href="#Measurement-Error-in-the-Independent-Variable" class="headerlink" title="Measurement Error in the Independent Variable"></a>Measurement Error in the Independent Variable</h4><ul>
<li>è‡ªå˜é‡çš„æµ‹é‡è¯¯å·®</li>
<li>Consider a simple regression model:$$y&#x3D;\beta_0+\beta_1 x_1^*+u$$We assume it satisfies the Gauss-Markov assumptions.</li>
</ul>
<ul>
<li>We do not observe $x_1^*$. Instead, we have a measure of $x_1^*$; call it $x_1$</li>
<li>The measurement error$$e_1&#x3D;x_1-x_1^*$$Assume $E\left(e_1\right)&#x3D;0$.</li>
</ul>
<ul>
<li>Plug in $x_1^*&#x3D;x_1-e_1$$$y&#x3D;\beta_0+\beta_1 x_1+(u-\beta_1 e_1)$$</li>
<li>To derive the properties of the OLS estimators, we need assumptions.</li>
<li>First, assume that:$$E(u \mid x_1^*, x_1)&#x3D;0$$This implies $E(y \mid x_1^*, x_1)&#x3D;E(y \mid x_1^*): x_1$ does not affect $y$ after $x_1^*$ has been controlled for.</li>
</ul>
<ul>
<li>Next, we consider two (mutually exclusive) cases about how the measurement error is correlated with $x$</li>
</ul>
<ol>
<li>$\operatorname{Cov}(x_1, e_1)&#x3D;0$</li>
<li>$\operatorname{Cov}(x_1^*, e_1)&#x3D;0$</li>
</ol>
<h5 id="Case-1ï¼š-operatorname-Cov-x-1-e-1-x3D-0"><a href="#Case-1ï¼š-operatorname-Cov-x-1-e-1-x3D-0" class="headerlink" title="Case 1ï¼š$\operatorname{Cov}(x_1, e_1)&#x3D;0$"></a>Case 1ï¼š$\operatorname{Cov}(x_1, e_1)&#x3D;0$</h5><ul>
<li>Plug in $x_1^*&#x3D;x_1-e_1$$$y&#x3D;\beta_0+\beta_1 x_1+(u-\beta_1 e_1)$$</li>
<li>Then $E\left(u-\beta_1 e_1 \mid x_1\right)&#x3D;0$, so the OLS estimator of the slope coefficient of $x_1$ in the above model gives us unbiased and consistent estimator of $\beta_1$.</li>
<li>If $u$ is uncorrelated with $e_1$, then $\operatorname{Var}\left(u-\beta_1 e_1\right)&#x3D;\sigma_u^2+\beta_1^2 \sigma_{e_1}^2$.</li>
<li>ä¸€è‡´æ€§å’Œæ— åæ€§ä»ç„¶æˆç«‹</li>
</ul>
<h5 id="Case-2ï¼š-operatorname-Cov-x-1-e-1-x3D-0"><a href="#Case-2ï¼š-operatorname-Cov-x-1-e-1-x3D-0" class="headerlink" title="Case 2ï¼š$\operatorname{Cov}(x_1^*, e_1)&#x3D;0$"></a>Case 2ï¼š$\operatorname{Cov}(x_1^*, e_1)&#x3D;0$</h5><ul>
<li>The classical <strong>errors-in-variables ï¼ˆCEVï¼‰</strong> assumption is that $e_1$ is uncorrealted with the unobserved variables.</li>
<li>Idea: the two components of $x_1$ is uncorrelated$$x_1&#x3D;x_1^*+e_1$$</li>
<li>Plug in $x_1^*&#x3D;x_1-e_1$$$y&#x3D;\beta_0+\beta_1 x_1+(u-\beta_1 e_1)$$</li>
<li>Then$$\operatorname{Cov}(u-\beta_1 e_1, x_1)&#x3D;-\beta_1 \operatorname{Cov}(x_1, e_1)&#x3D;-\beta_1 \sigma_{e_1}^2 \neq 0$$</li>
<li>æ­¤æ—¶ä¸€è‡´æ€§å’Œæ— åæ€§éƒ½ç ´åäº†</li>
<li>The probability limit of $\hat{\beta}_1$<ul>
<li>$$\operatorname{plim}(\hat{\beta_1}) &#x3D;\beta_1+\frac{\operatorname{Cov}(x_1, u-\beta_1 e_1)}{\operatorname{Var}(x_1)}&#x3D;\beta_1-\frac{\beta_1 \sigma_{e_1}^2}{\sigma_{x_1}^{2 *}+\sigma_{e_1}^2}&#x3D;\beta_1(\frac{\sigma_{x_1}^{2 *}}{\sigma_{x_1}^{2 *}+\sigma_{e_1}^2}) $$</li>
<li>$\operatorname{plim}(\hat{\beta}_1)$ is closer to zero than $\beta_1$.</li>
</ul>
</li>
<li>This is called the attenuation bias in OLS due to CEV</li>
<li>If the variance of $x_1^*$ is large relative to the variance in the measurement error, then the inconsistency in OLS will be small.</li>
</ul>
<h5 id="Case-3ï¼š-operatorname-Cov-x-1-e-1-x3D-0-text-and-operatorname-Cov-x-1-e-1-x3D-0"><a href="#Case-3ï¼š-operatorname-Cov-x-1-e-1-x3D-0-text-and-operatorname-Cov-x-1-e-1-x3D-0" class="headerlink" title="Case 3ï¼š$\operatorname{Cov}(x_1, e_1)&#x3D;0 \text{ and } \operatorname{Cov}(x_1^*, e_1)&#x3D;0$"></a>Case 3ï¼š$\operatorname{Cov}(x_1, e_1)&#x3D;0 \text{ and } \operatorname{Cov}(x_1^*, e_1)&#x3D;0$</h5><ul>
<li>è¿™ç§æƒ…å†µä¸‹ï¼ŒOLSå‡ ä¹ä¸€å®šä¼šé€ æˆæ— åæ€§å’Œä¸€è‡´æ€§å¤±æ•ˆã€‚</li>
</ul>
<h2 id="Ch15-Instrumental-Variable"><a href="#Ch15-Instrumental-Variable" class="headerlink" title="Ch15 Instrumental Variable"></a>Ch15 Instrumental Variable</h2><h3 id="IV-Estimator"><a href="#IV-Estimator" class="headerlink" title="IV Estimator"></a>IV Estimator</h3><h4 id="Omitted-Variable-bias"><a href="#Omitted-Variable-bias" class="headerlink" title="Omitted Variable bias"></a>Omitted Variable bias</h4><ul>
<li>$$\log (\text { wage })&#x3D;\beta_0+\beta_1 e d u c+\beta_2 a b i l+e$$</li>
<li>In this model, assume that $E(e \mid e d u c, a b i l)&#x3D;0$</li>
<li>åªæƒ³ä¸€è‡´åœ°ä¼°è®¡ $\beta_1$ ä¸åœ¨æ„ $\beta_2$.</li>
</ul>
<ul>
<li>å‡è®¾æ²¡æœ‰abliçš„æ•°æ®ï¼Œåªè¿›è¡Œä¸‹é¢å›å½’$$y&#x3D;\beta_0+\beta e d u c+u$$where $u&#x3D;\gamma a b i l+e$.</li>
<li>Note that $E(u \mid e d u c)&#x3D;E\left(\beta_2 a b i l+e \mid e d u c\right)&#x3D;\beta_2 E(a b i l \mid e d u c)$. If $E(a b i l)$ changes when educ changes, then the zero conditional mean assumption is not satisfied.</li>
</ul>
<ul>
<li>$$\begin{aligned}<br>\hat{\beta_{OLS}} &amp; &#x3D;\frac{\sum_{i&#x3D;1}^N(y_i-\bar{y})(x_i-\bar{x})}{\sum_{i&#x3D;1}^N(x_i-\bar{x})^2} &#x3D;\beta+\frac{\sum_{i&#x3D;1}^N(x_i-\bar{x}) u_i}{\sum_{i&#x3D;1}^N(x_i-\bar{x})^2} \\<br>\hat{\beta_{OLS}} &amp; \stackrel{plim}{\longrightarrow} \beta+\frac{\operatorname{cov}(x, u)}{\operatorname{var}(x)} .<br>\end{aligned}$$</li>
<li>Since $E(u \mid x) \neq 0, E\left(\hat{\beta}_{O L S}\right) \neq \beta$, OLS is not unbiased</li>
<li>Since $\operatorname{cov}(x, u) \neq 0$, OLS is not consistent.</li>
<li><strong>æ­¤æ—¶æ— åæ€§å’Œä¸€è‡´æ€§éƒ½ä¸æ»¡è¶³</strong></li>
</ul>
<h4 id="Instrumental-Variable-ï¼ˆIVï¼‰"><a href="#Instrumental-Variable-ï¼ˆIVï¼‰" class="headerlink" title="Instrumental Variable ï¼ˆIVï¼‰"></a>Instrumental Variable ï¼ˆIVï¼‰</h4><ul>
<li>ç”¨zæ¥æ›¿æ¢x</li>
<li>$$y&#x3D;\beta_0+\beta_1 x+u$$</li>
<li>ä»ç„¶å¯ä»¥ä¿è¯ $E(u)&#x3D;0$,å› ä¸ºå¯ä»¥è°ƒæ•´æˆªè·é¡¹</li>
</ul>
<ul>
<li>With $E(u \mid x) \neq 0$, we no longer have $E(x u)&#x3D;0$. </li>
<li>Estimation idea: find another variable $z$, where$$\operatorname{Cov}(x, z) \neq 0 ; \quad \operatorname{Cov}(z, u)&#x3D;0$$<ul>
<li>$\operatorname{Cov}(z, u)$ implies$\operatorname{Cov}(z, u)&#x3D;E(u z)-E(z) E(u)&#x3D;E(u z)&#x3D;0$</li>
</ul>
</li>
</ul>
<ul>
<li><p>Use $E(u z)&#x3D;0$ and $E(u)&#x3D;0$ to find the sample analogue, $$E(u z)&#x3D;0 \quad \frac{1}{N} \sum_{i&#x3D;1}^N z_i \hat{u_i}&#x3D;0 $$$$E(u)&#x3D;0 \quad \frac{1}{N} \sum_{i&#x3D;1}^N \hat{u_i}&#x3D;0$$and solve the equations.</p>
</li>
<li><p>$$\begin{aligned}<br>\hat{\beta_1}^{I V} &amp; &#x3D;\frac{\sum_{i&#x3D;1}^N\left(y_i-\bar{y}\right)\left(z_i-\bar{z}\right)}{\sum_{i&#x3D;1}^N\left(x_i-\bar{x}\right)\left(z_i-\bar{z}\right)} \\<br>\hat{\beta_0}^{I V} &amp; &#x3D;\bar{y}-\hat{\beta_1}^{I V} \bar{x}<br>\end{aligned}$$</p>
</li>
<li><p>æŠŠè¿™ä¸ªå«åš IV estimator</p>
</li>
<li><p>OLSæ±‚çš„å€¼æ˜¯ä¸€ç§ç‰¹æ®Šçš„IV estimatorï¼Œå½“z&#x3D;xæ—¶ã€‚</p>
</li>
</ul>
<h4 id="Assumptions-on-IV"><a href="#Assumptions-on-IV" class="headerlink" title="Assumptions on IV"></a>Assumptions on IV</h4><ul>
<li>Instrument relevance: <ul>
<li>ç›¸å…³æ€§</li>
<li>$\operatorname{Cov}(x, z) \neq 0: z$ is relevant for explaining variation in $x$. </li>
<li>$x&#x3D;\pi_0+\pi_1 z+v$, è¿›è¡Œé›¶æ£€éªŒ $H_0: \pi_1&#x3D;0$.</li>
</ul>
</li>
<li>Instrument exogeneity: <ul>
<li>å¤–ç”Ÿæ€§</li>
<li>$\operatorname{Cov}(u, z)&#x3D;0$ : ä¿è¯äº†ä¸€è‡´æ€§. </li>
<li>ä¸èƒ½ç›´æ¥ä»æ•°æ®ä¸­æ£€éªŒï¼Œéœ€è¦æ ¹æ®é‡‘èç†è®ºã€‚</li>
</ul>
</li>
</ul>
<h3 id="Properties-and-Inference-with-the-IV-Estimator"><a href="#Properties-and-Inference-with-the-IV-Estimator" class="headerlink" title="Properties and Inference with the IV Estimator"></a>Properties and Inference with the IV Estimator</h3><ul>
<li>ä¸€è‡´æ€§æ»¡è¶³</li>
<li>æ— åæ€§ä¸æ»¡è¶³<ul>
<li>consider the expectation of $\hat{\beta_1}^{IV}$ conditional on $z$$$E(\hat{\beta_1}^{I V})&#x3D;\beta+E(\frac{\sum_{i&#x3D;1}^N(z_i-\bar{z}) u_i}{\sum_{i&#x3D;1}^N(x_i-\bar{x})(z_i-\bar{z})})&#x3D;\beta+E(E[\frac{\sum_{i&#x3D;1}^n(z_i-\bar{z}) u_i}{\sum_{i&#x3D;1}^N(x_i-\bar{x})(z_i-\bar{z})} \mid z])$$</li>
<li>ç”±äºxä¸æ˜¯å¸¸æ•°ï¼Œå› æ­¤ä¸èƒ½è¿›ä¸€æ­¥åŒ–ç®€</li>
</ul>
</li>
<li>æ–¹å·®ï¼š<ul>
<li>å¢åŠ å‡è®¾$$E[u^2 \mid z]&#x3D;\sigma^2$$</li>
<li>åœ¨å¦‚ä¸Šå‡è®¾æƒ…å†µä¸‹ï¼š$$AVar(\hat{\beta_1}^{IV})&#x3D;\frac{\sigma^2}{N \sigma_x^2 \rho_{x, z}^2}$$</li>
<li>å…¶ä¸­ $\sigma_x^2$ æ˜¯æ€»ä½“xçš„æ–¹å·®ï¼Œ$\sigma^2$ æ˜¯æ€»ä½“uçš„æ–¹å·®ï¼Œ $\rho_{x,z}^2$ æ˜¯æ€»ä½“xå’Œzçš„ç›¸å…³æ€§</li>
<li>The asymptotic variance of $\hat{\beta}_1^{I V}$ is </li>
<li>$$\widehat{AVar}(\hat{\beta_1}^{IV})&#x3D;\frac{\hat{\sigma}^2}{SST_xR_{x,z}^2}$$<ul>
<li>where $S S T_x&#x3D;\sum_{i&#x3D;1}^n(x_i-\bar{x})^2$, and $R_{x, z}^2$ is the R-squared of $x_i$ on $z_i$.</li>
</ul>
</li>
</ul>
<ul>
<li>Note that the variance of the OLS estimator is </li>
<li>$$\widehat{\operatorname{Var}}(\hat{\beta}_1^{O L S})&#x3D;\frac{\hat{\sigma}^2}{S S T_x}$$</li>
<li><strong>So the IV estimator has a larger variance.</strong></li>
</ul>
<ul>
<li>If $x$ an $z$ are only slightly correlated, then $R_{x, z}^2$ can be small, and this translate into a large sampling variance of the IV estimator.</li>
</ul>
</li>
</ul>
<h3 id="Two-Stage-Least-Squares"><a href="#Two-Stage-Least-Squares" class="headerlink" title="Two Stage Least Squares"></a>Two Stage Least Squares</h3><h4 id="Multiple-Instrumental-Variables"><a href="#Multiple-Instrumental-Variables" class="headerlink" title="Multiple Instrumental Variables"></a>Multiple Instrumental Variables</h4><ul>
<li>æœ‰å¯èƒ½ä¸åªä¸€ä¸ªIV</li>
<li>è€ƒè™‘æ¨¡å‹$$y&#x3D;\beta_0+\beta_1 x_1+\beta_2 x_2+u$$</li>
<li>Assume $x_1$ is endogenous and has two IVs: $z_1$ and $z_2$. Assume $x_2$ is exogenous.</li>
<li>Two-stage least squares ï¼ˆ2SLSï¼‰: ä½¿ç”¨ä¸¤ä¸ªIVçš„çº¿æ€§ç»„åˆæ¥æ„é€ æ–°çš„IV</li>
</ul>
<h4 id="2SLS"><a href="#2SLS" class="headerlink" title="2SLS"></a>2SLS</h4><ul>
<li><p>æ¥ç€ä¸Šé¢çš„ä¾‹å­</p>
</li>
<li><p>The steps of 2SLS</p>
<ol>
<li>Stage 1: estimate ï¼ˆusing OLSï¼‰$$x_1&#x3D;\alpha_0+\alpha_1 z_1+\alpha_2 z_2+\alpha_3 x_2+v$$and calculate $\hat{x}_1$.<ul>
<li>è¿™ä¸ªé˜¶æ®µçš„å›å½’éœ€è¦åŒ…å«æ‰€æœ‰çš„å¤–ç”Ÿå˜é‡</li>
</ul>
</li>
<li>Stage 2: use $\hat{x}_1$ as an IV for $x_1$. Or directly estimate$$y&#x3D;\beta_0+\beta_1 \hat{x}_1+\beta_2 x_2+u$$</li>
</ol>
</li>
<li><p>å¤šä¸ªå†…ç”Ÿå˜é‡æ—¶ï¼š</p>
<ul>
<li><p>è€ƒè™‘æœ‰ä¸¤ä¸ªå†…ç”Ÿå˜é‡çš„æ¨¡å‹:$$y&#x3D;\beta_0+\beta_1 x_1+\beta_2 x_2+\beta_3 x_3+u$$where $x_1$ and $x_2$ are endogenous ï¼ˆwhose IV are $z_1$ and $z_2$ï¼‰, $x_3$ is exogenous.</p>
</li>
<li><p>In the first stage, we need to include all instruments and exogenous variables on the right hand side</p>
</li>
<li><p>$$x_1&#x3D;\alpha_0+\alpha_1 z_1+\alpha_2 z_2+\alpha_3 x_3+v$$$$x_2&#x3D;\gamma_0+\gamma_1 z_1+\gamma_2 z_2+\gamma_3 x_3+v$$</p>
</li>
</ul>
</li>
<li><p><strong>IVçš„æ•°é‡åº”è¯¥å¤§äºç­‰äºå†…ç”Ÿå˜é‡çš„æ•°é‡</strong></p>
</li>
</ul>
<h3 id="Issues-with-IV"><a href="#Issues-with-IV" class="headerlink" title="Issues with IV"></a>Issues with IV</h3><h4 id="Sample-size"><a href="#Sample-size" class="headerlink" title="Sample size"></a>Sample size</h4><ul>
<li>éœ€è¦ä¸€ä¸ªå¤§æ ·æœ¬ï¼Œå› ä¸ºåœ¨2SLSçš„ç¬¬ä¸€æ­¥ä¸­ï¼Œ$x&#x3D;\alpha_0+\alpha_1z+e$</li>
<li>å…¶ä¸­çš„$\alpha_0+Î±_1z$ æ˜¯ä¸uæ— å…³çš„ï¼Œeæ˜¯ä¸uç›¸å…³çš„</li>
<li>åœ¨2SLSç¬¬äºŒæ­¥ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ç”¨$\alpha_0+\alpha_1 z$ æ¥è¡¨ç¤ºx,ä½†å®é™…ä¸Šæ˜¯ç”¨ $\hat{\alpha}_0+\hat{\alpha}_1 z$. åé¡¹å¯èƒ½åŒ…å«å…³äºuçš„ä¿¡æ¯ã€‚</li>
<li>å› æ­¤éœ€è¦å¤§æ ·æœ¬ã€‚</li>
</ul>
<h4 id="Weak-instruments"><a href="#Weak-instruments" class="headerlink" title="Weak instruments"></a>Weak instruments</h4><ul>
<li>Weak instruments: low correlation between $x$ and $z$</li>
<li>Suppose there is some small correlated between $u$ and $z$$$\begin{aligned}\operatorname{plim}(\hat{\beta}_1^{I V}) &amp; &#x3D;\beta_1+\frac{\operatorname{Cov}(z, u)}{\operatorname{Cov}(z, x)} \\ &amp; &#x3D;\beta_1+\frac{\operatorname{Corr}(z, u)}{\operatorname{Corr}(z, x)} \cdot \frac{\sigma_u}{\sigma_x},\end{aligned}$$where $\sigma_u$ and $\sigma_x$ are the standard deviations of $u$ and $x$ in the population respectively.</li>
<li>We can show that$$\operatorname{plim}(\hat{\beta}_1^{O L S})&#x3D;\beta_1+\operatorname{Corr}(x, u) \cdot \frac{\sigma_u}{\sigma_x}$$</li>
<li>If $\operatorname{Corr}(z, x)$ is small enough, then even if $\operatorname{Corr}(z, u)$ is small, the IV estimator could result in larger asymptotic bias than the OLS estimator.</li>
<li>å¯¹äºæ— åæ€§åœ¨å°æ ·æœ¬ä¸‹ä¹Ÿæœ‰å½±å“ã€‚</li>
</ul>
<h5 id="æ£€éªŒå¼±ç›¸å…³"><a href="#æ£€éªŒå¼±ç›¸å…³" class="headerlink" title="æ£€éªŒå¼±ç›¸å…³"></a>æ£€éªŒå¼±ç›¸å…³</h5><ul>
<li>$$y&#x3D;\beta_0+\beta_1 x_1+\beta_2 x_2+u$$</li>
<li>Assume $x_1$ is endogenous with two instrumental variables $z_1$ and $z_2$, and $x_2$ is exogenous.</li>
<li>Estimate$$x_1&#x3D;\alpha_0+\alpha_1 z_1+\alpha_2 z_2+\alpha_3 x_2+e$$</li>
<li>Test $H_0: \alpha_1&#x3D;\alpha_2&#x3D;0$</li>
<li>å½“F-statå¤§äº10çš„æ—¶å€™å¯ä»¥è¯´æ²¡æœ‰å¼±ç›¸å…³æ€§</li>
</ul>
<h4 id="Testing-for-endogeneity"><a href="#Testing-for-endogeneity" class="headerlink" title="Testing for endogeneity"></a>Testing for endogeneity</h4><ul>
<li>æ£€éªŒxä¸uæ˜¯å¦ç›¸å…³</li>
<li>Suppose $x_1$ is endogenous, and the IV is $z$$$y&#x3D;\beta_0+\beta_1 x_1+\beta_2 x_2+u$$</li>
<li>First stage: $x_1&#x3D;\alpha_0+\alpha_1 z+\alpha_2 x_2+v$. If $x_1$ is correlated with $u$, then it must be $v$ is correlated with $u$. Estimate the equation to get $\hat{v}$.</li>
</ul>
<ul>
<li>Estimate $y&#x3D;\delta_0+\delta_1 x_1+\delta_2 x_2+\delta_3 \hat{v}+e$. Test $H_0: \delta_3&#x3D;0$.</li>
</ul>
<h4 id="Testing-overidentifying-restrictions"><a href="#Testing-overidentifying-restrictions" class="headerlink" title="Testing overidentifying restrictions"></a>Testing overidentifying restrictions</h4><ul>
<li><p>å½“zæ¯”xå¤šçš„æ—¶å€™ï¼Œå¯ä»¥æ¯”è¾ƒå¥½åœ°æ¨æ–­ $Cov(z,u)&#x3D;0$</p>
</li>
<li><p>Suppose $x_1$ is endogenous, and the IVs are $z_1$ and $z_2$.</p>
</li>
<li><p>Use either one of them, we calculate $\hat{\beta}_1^{I V 1}$ and $\hat{\beta}_1^{I V 2}$</p>
</li>
<li><p>If $\hat{\beta}_1^{I V 1}$ is very different from $\hat{\beta}_1^{I V 2}$, then at least one of them does not satisfy $\operatorname{Cov}(z, u)&#x3D;0$.</p>
</li>
<li><p>If they are close to each other, then it could be both satisfies $\operatorname{Cov}(z, u)&#x3D;0$, or neither.</p>
</li>
<li><p>å½“zå¾ˆå¤šçš„æ—¶å€™</p>
<ul>
<li>Testing overidentifying restrictions:<ol>
<li>ä½¿ç”¨2SLSä¼°è®¡ï¼Œå¹¶å¾—åˆ°2SLSæ®‹é¡¹ $\hat{u}_1$.</li>
<li>Regress $\hat{u}_1$ on all exogenous variables. Obtain the $R$-squared, say $R^2$.</li>
<li>è‹¥æ‰€æœ‰IVéƒ½ä¸ $u_1$ æ— å…³ï¼Œåˆ™ $N \cdot R^2 \sim \chi_q^2$, where $q$ is the number of instrumental variables from outside the model minus the total number of endogenous explanatory variables. </li>
<li>Reject $H_0$ if $N \cdot R^2$ exceeds the critical value.</li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="Ch17-Limited-Dependent-Variable-Models"><a href="#Ch17-Limited-Dependent-Variable-Models" class="headerlink" title="Ch17 Limited Dependent Variable Models"></a>Ch17 Limited Dependent Variable Models</h2><h3 id="Linear-Probability-model"><a href="#Linear-Probability-model" class="headerlink" title="Linear Probability model"></a>Linear Probability model</h3><h4 id="Limited-Dependent-Variable"><a href="#Limited-Dependent-Variable" class="headerlink" title="Limited Dependent Variable"></a>Limited Dependent Variable</h4><ul>
<li>å› å˜é‡åªèƒ½å–ç‰¹å®šå€¼</li>
<li>In the population, $y$ takes on two values: 0 and 1 . We are interested in how $x$ will affect $y$.</li>
<li>Suppose $x$ and $y$ has this linear relation:$$y&#x3D;\beta_0+\beta_1 x+u$$</li>
<li>Suppose $E(u \mid x)&#x3D;0$. Then$$E(y \mid x)&#x3D;P(y&#x3D;1 \mid x)&#x3D;\beta_0+\beta_1 x$$$\beta_1$ represents when $x$ increases by one unit, the impact on the probability that $y&#x3D;1$. In other words, $\beta_1.$ measures the marginal effect of $x$ on the probability that $y&#x3D;1$</li>
<li>yæ˜¯å¦æ˜¯äºŒå…ƒå˜é‡éƒ½ä¸å½±å“å¯¹è¿™ä¸ªæ¨¡å‹çš„è§£é‡Š<ul>
<li>Descriptiveï¼š $\beta_1$ is the expected difference in the probability that $y&#x3D;1$ if $x$ changes by one unit.</li>
<li>Causal: one unit increase in $x$ causes the probability of $y&#x3D;1$ to change by $\beta_1$ on average.</li>
</ul>
</li>
<li>è¿™ä¸ªæ¨¡å‹è¿åäº†åŒæ–¹å·®æ€§ï¼Œå› ä¸ºæ–¹å·®æ˜¯xçš„å‡½æ•°ï¼Œå¯ä»¥ç”¨FGLSæ¥ä¼°ç®—ã€‚</li>
</ul>
<h3 id="Non-linear-Model-and-Maximum-Likelihood"><a href="#Non-linear-Model-and-Maximum-Likelihood" class="headerlink" title="Non-linear Model and Maximum Likelihood"></a>Non-linear Model and Maximum Likelihood</h3><ul>
<li>Consider the following non-linear model:$$E(y \mid x)&#x3D;P(y&#x3D;1 \mid x)&#x3D;G(\beta_0+\beta_1 x)$$where $G$ is a function mapping values to the range of 0 and 1 , to make sure $E(y \mid x)$ belongs to 0 and 1 .</li>
<li>$G$ can have different functional forms. We consider two common ones:<ul>
<li>logistic function ï¼ˆlogitï¼‰<ul>
<li>$$G(z)&#x3D;\frac{\exp (z)}{1+\exp (z)}$$</li>
</ul>
</li>
<li>standard normal CDF ï¼ˆprobitï¼‰<ul>
<li>$$G(z)&#x3D;\Phi(z)$$</li>
</ul>
</li>
<li><img src="https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230409160715.png" alt="image.png"></li>
</ul>
</li>
</ul>
<h4 id="Properties-of-Logit-and-Probit"><a href="#Properties-of-Logit-and-Probit" class="headerlink" title="Properties of Logit and Probit"></a>Properties of Logit and Probit</h4><ul>
<li>yçš„è¿™ä¸¤ç§åˆ†å¸ƒå–å†³äºæ®‹é¡¹eçš„åˆ†å¸ƒ</li>
<li>Suppose random variable $e$ has a CDF:$$\operatorname{Pr}(e \leq z)&#x3D;G(z)$$Here $G(z)$ can be either logit or probit.</li>
<li>Let $y^*&#x3D;\beta_0+\beta_1 x+e$, where $e$ is independent of $x$.</li>
<li>$$\begin{aligned} P(y&#x3D;1 \mid x) &amp; &#x3D;P(y^*&gt;0 \mid x) &#x3D;P(\beta_0+\beta_1 x+e&gt;0 \mid x) \\ &amp; &#x3D;P(e&gt;-\beta_0-\beta_1 x) &#x3D;1-\operatorname{Pr}(e \leq-\beta_0-\beta_1 x) \\ &amp; &#x3D;1-G\left(-\beta_0-\beta_1 x\right)&#x3D;G\left(\beta_0+\beta_1 x\right) .\end{aligned}$$</li>
</ul>
<h4 id="Partial-effect-of-x-on-y"><a href="#Partial-effect-of-x-on-y" class="headerlink" title="Partial effect of x on y"></a>Partial effect of x on y</h4><ul>
<li>the marginal effect of $x$ on the probability that $y&#x3D;1$$$\frac{\partial p(x)}{\partial x_1}&#x3D;g(\beta_0+\beta_1 x) \beta_1$$where $p(x)&#x3D;P(y&#x3D;1 \mid x), g(z) \equiv \frac{d G}{d z}(z)$.<ul>
<li>å³å¯¹ä¸Šé¢çš„å¼å­æ±‚å¯¼</li>
</ul>
</li>
<li>When we have more than one independent variables:$$\frac{\partial p(x)}{\partial x_j}&#x3D;g(\beta_0+\boldsymbol{x} \boldsymbol{\beta}) \beta_j$$where $\boldsymbol{x} \boldsymbol{\beta}&#x3D;\beta_1 x_1+\ldots+\beta_k x_k$.</li>
</ul>
<ul>
<li>So the ratio of the partial effect of $x_j$ and $x_k$ is $\frac{\beta_j}{\beta_k}$.</li>
<li>è¿™ç§æ–¹æ³•å’ŒOLSæ¯”çš„å¼±ç‚¹åœ¨äºè¿™ä¸ªåå¯¼æ•°çš„å‰é¡¹gå–å†³äºx</li>
<li>è§£å†³åŠæ³•ï¼š<ol>
<li>æ‰¾ç‰¹æ®Šç‚¹ï¼Œä¾‹å¦‚å‡å€¼ç‚¹ã€‚<ul>
<li>Partial effect at the average:$$g(\hat{\beta_0}+\overline{\boldsymbol{x}}\hat{\beta_{\mathbf{1}}})&#x3D;g(\hat{\beta_0}+\hat{\beta_1} \bar{x_1}+\hat{\beta_2} \bar{x_2}+\cdot+\hat{\beta_k} \bar{x_k})$$</li>
</ul>
</li>
<li>æ±‚åå¯¼çš„å‡å€¼<ul>
<li>Average marginal effect:$$[N^{-1} \sum_{i&#x3D;1}^N g(\hat{\beta_0}+x_{i} \hat{\beta_1})] \hat{\beta_j}$$</li>
</ul>
</li>
</ol>
</li>
</ul>
<h4 id="Maximum-Likelihood-Estimation"><a href="#Maximum-Likelihood-Estimation" class="headerlink" title="Maximum Likelihood Estimation"></a>Maximum Likelihood Estimation</h4><ul>
<li><p>ä¼°è®¡ï¼šåœ¨æ ·æœ¬ä¸­è§‚å¯Ÿåˆ°æŸäº›å€¼æ—¶y&#x3D;1ï¼Œå¦å¤–ä¸€äº›å€¼æ—¶y&#x3D;0</p>
</li>
<li><p>ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ‰¾åˆ° $\beta$ ä½¿å¾—è¿™ç§æˆç«‹çš„æ¦‚ç‡æœ€å¤§ã€‚</p>
</li>
<li><p>ä¸‹é¢å°±æ˜¯ä¸€ä¸ªæ™®é€šçš„æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„è¿‡ç¨‹ï¼Œå½“ä½œæ¦‚ç»Ÿå¤ä¹ ï¼š</p>
</li>
<li><p>Suppose we have a random sample of size $N$. Fix every $x$ and $\beta$, the probability that $y&#x3D;1$ is:$$E(y \mid x)&#x3D;P(y&#x3D;1 \mid x)&#x3D;G(\beta_0+\beta_1 x) \equiv G(\beta x) $$</p>
</li>
<li><p>Then for any observation $y&#x3D;0$ or $y&#x3D;1$, its probability density function is:$$f(y \mid \beta x)&#x3D;G(\beta x)^y[1-G(\beta x)]^{(1-y)}$$</p>
</li>
<li><p>For a random sample, all observations are independent of each other. Then the probability that we observe the sample is: ï¼ˆ $i$ is the index for each observationï¼‰$$f({y_1, \ldots, y_N} \mid \beta x_i)&#x3D;\prod_{i&#x3D;1}^N[G(\beta x_i)]^{y_i}[1-G(\beta x_i)]^{(1-y_i)}$$</p>
</li>
</ul>
<ul>
<li>Maximum likelihood estimation (MLE): maximize the probability that we observe the data:$$\max_{\boldsymbol{\beta}} f({y_1, \ldots, y_N} \mid \beta x_i)&#x3D;\max_{\boldsymbol{\beta}} \prod_{i&#x3D;1}^N[G(\beta x_i)]^{y_i}[1-G(\beta \boldsymbol{x}_{\boldsymbol{i}})]^{(1-y_i)}$$</li>
</ul>
<ul>
<li>Take the natural logarithm and define:$$\ell_i(\beta)&#x3D;\log ([G(\beta x_i)]^{y_i}[1-G(\beta x_i)]^{(1-y_i)})&#x3D;y_i \log [G( x_i)]+(1-y_i) \log [1-G(\beta x_i)]$$</li>
<li>Then we can equivalently write:$$\max_\beta \sum_{i&#x3D;1}^N \ell_i(\beta)&#x3D;\max_{\boldsymbol{\beta}} \sum_{y_i&#x3D;1} \log [G(\beta x_{\boldsymbol{i}})]+\sum_{y_i&#x3D;0} \log [1-G(\beta x_{\boldsymbol{i}})]$$</li>
<li><strong>MLE is consistent and asymptotically efficient.</strong></li>
</ul>
<h4 id="MLE-and-OLS"><a href="#MLE-and-OLS" class="headerlink" title="MLE and OLS"></a>MLE and OLS</h4><ul>
<li>OLSç”¨æ¥ä¼°ç®—çº¿æ€§æ¨¡å‹</li>
<li>MLEç”¨æ¥ä¼°è®¡çº¿æ€§å’Œéçº¿æ€§æ¨¡å‹</li>
<li>å½“uæœä»æ­£æ€åˆ†å¸ƒæ—¶ï¼ŒMLEä¸OLSç»“æœç›¸åŒ</li>
</ul>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><ul>
<li><p>$\color{red}{\text{Law of Iterated Expectation:}}$</p>
<ul>
<li>$$\color{red}{E(y)&#x3D;E[E(y|x)]}$$</li>
</ul>
</li>
<li><p>Summation operation</p>
<ul>
<li>$$\sum_{i&#x3D;1}^N(x_i-\bar{x})(y_i-\bar{y})&#x3D;\sum_{i&#x3D;1}^N(x_i-\bar{x})y_i&#x3D;\sum_{i&#x3D;1}^N(x_iy_i-\bar{x}\bar{y})$$</li>
</ul>
</li>
<li><p>Variance</p>
<ul>
<li>$$\begin{aligned}<br>\operatorname{Var}(X+a) &amp; &#x3D;\operatorname{Var}(X) \\<br>\operatorname{Var}(a X) &amp; &#x3D;a^2 \operatorname{Var}(X) \\<br>\operatorname{Var}(X) &amp; &#x3D;\operatorname{Cov}(X, X) \\<br>\operatorname{Var}(a X+b Y) &amp; &#x3D;a^2 \operatorname{Var}(X)+b^2 \operatorname{Var}(Y)+2 a b \operatorname{Cov}(X, Y) \\<br>\operatorname{Var}\left(\sum_{i&#x3D;1}^N a_i X_i\right) &amp; &#x3D;\sum_{i, j&#x3D;1}^N a_i a_j \operatorname{Cov}(X_i, X_j) \\<br>&amp; &#x3D;\sum_{i&#x3D;1}^N a_i^2 \operatorname{Var}(X_i)+\sum_{i \neq j} a_i a_j \operatorname{Cov}(X_i, X_j) \\<br>&amp; &#x3D;\sum_{i&#x3D;1}^N a_i^2 \operatorname{Var}(X_i)+2 \sum_{i&#x3D;1}^N \sum_{j&#x3D;i+1}^N a_i a_j \operatorname{Cov}(X_i, X_j) .<br>\end{aligned}$$</li>
</ul>
</li>
</ul>

            </div>
            <hr />

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-large waves-effect waves-light red">èµ</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">éšç¼˜ä¹°å’–å•¡ï½</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <div id="wechat">
                        <img src="/medias/reward/Wechat_yyc.png" class="reward-img" alt="å¾®ä¿¡æ‰“èµäºŒç»´ç ">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>å¾®ä¿¡é‡Œç‚¹â€œå‘ç°â€->â€œæ‰«ä¸€æ‰«â€äºŒç»´ç ä¾¿å¯æŸ¥çœ‹åˆ†äº«ã€‚</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            

    <div class="reprint" id="reprint-statement">
        <p class="reprint-tip">
            <i class="fa fa-exclamation-triangle"></i>&nbsp;&nbsp;
            <span>è½¬è½½è§„åˆ™</span>
        </p>
        
            <div class="center-align">
                <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                    <img alt=""
                         style="border-width:0"
                         src="https://i.creativecommons.org/l/by/4.0/88x31.png"/>
                </a>
            </div>
            <br/>
            <span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text"
                  property="dct:title" rel="dct:type">
                    ã€Šè®¡é‡ç»æµå­¦ç¬”è®°ã€‹
                </span> ç”±
            <a xmlns:cc="http://creativecommons.org/ns#" href="/2023/04/10/ji-liang-jing-ji-xue-bi-ji/" property="cc:attributionName"
               rel="cc:attributionURL">
                Frank Yu
            </a> é‡‡ç”¨
            <a rel="license noopener" target="_blank" href="https://creativecommons.org/licenses/by/4.0/deed.zh">
                çŸ¥è¯†å…±äº«ç½²å 4.0 å›½é™…è®¸å¯åè®®
            </a>è¿›è¡Œè®¸å¯ã€‚
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>


        </div>
    </div>

    

    

    

    
    <div class="livere-card card" data-aos="fade-up">
    <!-- æ¥å¿…åŠ›Cityç‰ˆå®‰è£…ä»£ç  -->
    <div id="lv-container" class="card-content" data-id="city" data-uid="MTAyMC81ODE5NC8zNDY1Nw">
        <script type="text/javascript">
            (function (d, s) {
                let j, e = d.getElementsByTagName(s)[0];
                if (typeof LivereTower === 'function') {
                    return;
                }

                j = d.createElement(s);
                j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
                j.async = true;

                e.parentNode.insertBefore(j, e);
            })(document, 'script');
        </script>
        <noscript>ä¸ºæ­£å¸¸ä½¿ç”¨æ¥å¿…åŠ›è¯„è®ºåŠŸèƒ½è¯·æ¿€æ´»JavaScriptã€‚</noscript>
    </div>
    <!-- Cityç‰ˆå®‰è£…ä»£ç å·²å®Œæˆ -->
</div>
    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/2023/04/13/xiao-chou-ying-ping/">
                    <div class="card-image">
                        
                        <img src="https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/fcb7da73360f34fb1343351a207e4fe22cc3a468.jpg@942w_1676h_progressive.jpg" class="responsive-img" alt="ã€Šå°ä¸‘ã€‹å½±è¯„">
                        
                        <span class="card-title">ã€Šå°ä¸‘ã€‹å½±è¯„</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å°ä¸‘å½±è¯„+chatgptéƒ¨åˆ†å†…å®¹
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2023-04-13
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Waffle/" class="post-category" target="_blank">
                                    Waffle
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Movie/" target="_blank">
                        <span class="chip bg-color">Movie</span>
                    </a>
                    
                    <a href="/tags/Joker/" target="_blank">
                        <span class="chip bg-color">Joker</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/03/25/tou-zi-xue-bi-ji/">
                    <div class="card-image">
                        
                        <img src="https://cdn.jsdelivr.net/gh/ldvyyc/ImgBed/20230326155842.png" class="responsive-img" alt="æŠ•èµ„å­¦ç¬”è®°">
                        
                        <span class="card-title">æŠ•èµ„å­¦ç¬”è®°</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            å¤ä¹ ï¼Œæœ±è‹±å§¿è€å¸ˆæŠ•èµ„å­¦è¯¾ä»¶ï¼Œè®°å½•ç¬”è®°
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2023-03-25
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/learning/" class="post-category" target="_blank">
                                    learning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Finance/" target="_blank">
                        <span class="chip bg-color">Finance</span>
                    </a>
                    
                    <a href="/tags/Notes/" target="_blank">
                        <span class="chip bg-color">Notes</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'æ¥æº: Frank's Blog<br />'
            + 'ä½œè€…: Frank Yu<br />'
            + 'é“¾æ¥: <a href="' + url + '">' + url + '</a><br />'
            + 'æœ¬æ–‡ç« è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ï¼Œä»»ä½•å½¢å¼çš„è½¬è½½éƒ½è¯·æ³¨æ˜å‡ºå¤„ã€‚';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () { bodyElement.removeChild(newdiv); }, 200);
    });
</script>

    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fa fa-list"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            // headingsOffset: -205,
            headingSelector: 'h1, h2, h3, h4, h5, h6'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h1, h2, h3, h4, h5, h6').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).slideUp(500);
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).slideDown(500);
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>
    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>

<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>
<!-- ä»£ç è¯­è¨€ -->
<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>
<!-- ä»£ç å—å¤åˆ¶ -->
<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>
<script type="text/javascript" src="/libs/codeBlock/clipboard.min.js"></script>
<!-- ä»£ç å—æ”¶ç¼© -->
<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script> 
<!-- ä»£ç å—æŠ˜è¡Œ -->
<style type="text/css">code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }</style>


    <footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            &copy; 2023-2023 Yu YuChen ç‰ˆæƒæ‰€æœ‰

            
            &nbsp;<i class="fa fa-area-chart"></i>&nbsp;ç«™ç‚¹æ€»å­—æ•°:&nbsp;
            <span class="white-color">80.2k</span>
            

            <br>
            <span id="sitetime"></span>

            
            
            <br>
            
            <span id="busuanzi_container_site_pv" style='display:none'>
                <i class="fa fa-heart-o"></i>
                æœ¬ç«™æ€»è®¿é—®é‡ <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
            <span id="busuanzi_container_site_uv" style='display:none'>
                äººæ¬¡,&nbsp;è®¿å®¢æ•° <span id="busuanzi_value_site_uv" class="white-color"></span> äºº.
            </span>
            
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/ldvyyc" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:yu-yc20@mails.tsinghua.edu.cn" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="https://zhihu.com/people/ldvyyc" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„çŸ¥ä¹" data-position="top" data-delay="50">
        <i class="fa fa-inverse">çŸ¥</i>
    </a>



    <a href="http://wpa.qq.com/msgrd?v=3&uin=2992197817&site=qq&menu=yes" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„çŸ¥ä¹" data-position="top" data-delay="50">
        <i class="fa fa-qq"></i>
    </a>





    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS è®¢é˜…" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>
</div>
    </div>
</footer>

<div class="progress-bar"></div>

<!-- ä¸è’œå­è®¡æ•°åˆå§‹å€¼çº æ­£ -->
<script>
    $(document).ready(function () {

        var int = setInterval(fixCount, 50);
        var pvcountOffset = 80000;
        var uvcountOffset = 20000;

        function fixCount() {
            if (document.getElementById("busuanzi_container_site_pv").style.display != "none") {
                $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()));
                clearInterval(int);
            }
            if ($("#busuanzi_container_site_pv").css("display") != "none") {
                $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html())); // åŠ ä¸Šåˆå§‹æ•°æ® 
                clearInterval(int);
            }
        }
    });
</script>

<script language=javascript>
    function siteTime() {
        window.setTimeout("siteTime()", 1000);
        var seconds = 1000;
        var minutes = seconds * 60;
        var hours = minutes * 60;
        var days = hours * 24;
        var years = days * 365;
        var today = new Date();
        var todayYear = today.getFullYear();
        var todayMonth = today.getMonth() + 1;
        var todayDate = today.getDate();
        var todayHour = today.getHours();
        var todayMinute = today.getMinutes();
        var todaySecond = today.getSeconds();
        /* Date.UTC() -- è¿”å›dateå¯¹è±¡è·ä¸–ç•Œæ ‡å‡†æ—¶é—´(UTC)1970å¹´1æœˆ1æ—¥åˆå¤œä¹‹é—´çš„æ¯«ç§’æ•°(æ—¶é—´æˆ³)
        year - ä½œä¸ºdateå¯¹è±¡çš„å¹´ä»½ï¼Œä¸º4ä½å¹´ä»½å€¼
        month - 0-11ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„æœˆä»½
        day - 1-31ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„å¤©æ•°
        hours - 0(åˆå¤œ24ç‚¹)-23ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„å°æ—¶æ•°
        minutes - 0-59ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„åˆ†é’Ÿæ•°
        seconds - 0-59ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„ç§’æ•°
        microseconds - 0-999ä¹‹é—´çš„æ•´æ•°ï¼Œåšä¸ºdateå¯¹è±¡çš„æ¯«ç§’æ•° */
        var t1 = Date.UTC(2023, 03, 10, 00, 00, 00); //åŒ—äº¬æ—¶é—´2018-2-13 00:00:00
        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
        var diff = t2 - t1;
        var diffYears = Math.floor(diff / years);
        var diffDays = Math.floor((diff / days) - diffYears * 365);
        var diffHours = Math.floor((diff - (diffYears * 365 + diffDays) * days) / hours);
        var diffMinutes = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours) / minutes);
        var diffSeconds = Math.floor((diff - (diffYears * 365 + diffDays) * days - diffHours * hours - diffMinutes * minutes) / seconds);
        document.getElementById("sitetime").innerHTML = "æœ¬ç«™å·²è¿è¡Œ " + diffYears + " å¹´ " + diffDays + " å¤© " + diffHours + " å°æ—¶ " + diffMinutes + " åˆ†é’Ÿ " + diffSeconds + " ç§’";
    }/*å› ä¸ºå»ºç«™æ—¶é—´è¿˜æ²¡æœ‰ä¸€å¹´ï¼Œå°±å°†ä¹‹æ³¨é‡Šæ‰äº†ã€‚éœ€è¦çš„å¯ä»¥å–æ¶ˆ*/
    siteTime();
</script>

    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
});
</script>
    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <script type="text/javascript"> var OriginTitile = document.title, st; document.addEventListener("visibilitychange", function () { document.hidden ? (document.title = "Î£(ã£ Â°Ğ” Â°;)ã£å–”å“Ÿï¼Œå´©æºƒå•¦ï¼", clearTimeout(st)) : (document.title = "Ï†(ã‚œâ–½ã‚œ*)â™ªå’¦ï¼Œåˆå¥½äº†ï¼", st = setTimeout(function () { document.title = OriginTitile }, 3e3)) })
    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->



    
    <script src="/libs/others/firework.js"></script>    
    

    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    <!-- é›ªèŠ±ç‰¹æ•ˆ -->
    

</body>

</html>